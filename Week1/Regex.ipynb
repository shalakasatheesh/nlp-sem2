{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b856398895df18672cfa9f79ce5d925e",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Wednesday at 15:00</b>.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to two people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the UID of each team member into the variables. \n",
    "If you work alone please leave the second variable empty.\n",
    "'''\n",
    "member1 = 'Syed Mushrraf Ali (sali2s, 9040658)'\n",
    "member2 = 'Shalaka Satheesh (ssathe2s, 9040760)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "117fabf05b9fc1ac48e6c7d3ee164856",
     "grade": false,
     "grade_id": "wiki_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Regular expressions\n",
    "\n",
    "Consider the excerpt from the [Wikipedia article](https://en.wikipedia.org/wiki/Regular_expression) about **regular expressions**.\n",
    "\n",
    "We now want to perform some tasks with the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8f2424a98719a5de6abc349a16abc47",
     "grade": false,
     "grade_id": "wiki_text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Regular expressions originated in 1951, when mathematician Stephen Cole Kleene described regular languages \\\n",
    "using his mathematical notation called regular events.[4][5] These arose in theoretical computer science, \\\n",
    "in the subfields of automata theory (models of computation) and the description and \\\n",
    "classification of formal languages. Other early implementations of pattern matching include the SNOBOL language, \\\n",
    "which did not use regular expressions, but instead its own pattern matching constructs.\n",
    "\n",
    "Regular expressions entered popular use from 1968 in two uses: pattern matching in a text editor[6] \\\n",
    "and lexical analysis in a compiler.[7] Among the first appearances of regular expressions in program \\\n",
    "form was when Ken Thompson built Kleene's notation into the editor QED as a means to match patterns in \\\n",
    "text files.[6][8][9][10] For speed, Thompson implemented regular expression matching by \\\n",
    "just-in-time compilation (JIT) to IBM 7094 code on the Compatible Time-Sharing System, an important \\\n",
    "early example of JIT compilation.[11] He later added this capability to the Unix editor ed, which eventually \\\n",
    "led to the popular search tool grep's use of regular expressions (\"grep\" is a word derived from the command \\\n",
    "for regular expression searching in the ed editor: g/re/p meaning \"Global search for Regular Expression and \\\n",
    "Print matching lines\"[12]). Around the same time when Thompson developed QED, a group of researchers \\\n",
    "including Douglas T. Ross implemented a tool based on regular expressions that is used for \\\n",
    "lexical analysis in compiler design.[7]\n",
    "\n",
    "Many variations of these original forms of regular expressions were used in Unix[10] programs at Bell Labs \\\n",
    "in the 1970s, including vi, lex, sed, AWK, and expr, and in other programs such as Emacs. Regexes were \\\n",
    "subsequently adopted by a wide range of programs, with these early forms standardized in the POSIX.2 standard in 1992.\n",
    "\n",
    "In the 1980s the more complicated regexes arose in Perl, which originally derived from a regex library \\\n",
    "written by Henry Spencer (1986), who later wrote an implementation of Advanced Regular Expressions \\\n",
    "for Tcl.[13] The Tcl library is a hybrid NFA/DFA implementation with improved performance characteristics. \\\n",
    "Software projects that have adopted Spencer's Tcl regular expression implementation include PostgreSQL.[14] \\\n",
    "Perl later expanded on Spencer's original library to add many new features.[15] Part of the effort in the \\\n",
    "design of Raku is to improve Perl's regex integration, and to increase their scope and capabilities to \\\n",
    "allow the definition of parsing expression grammars.[16] The result is a mini-language called Raku rules, \\\n",
    "which are used to define Raku grammar as well as provide a tool to programmers in the language. \\\n",
    "These rules maintain existing features of Perl 5.x regexes, but also allow BNF-style definition of a \\\n",
    "recursive descent parser via sub-rules.\n",
    "\n",
    "The use of regexes in structured information standards for document and database modeling started in the 1960s \\\n",
    "and expanded in the 1980s when industry standards like ISO SGML (precursored by ANSI \"GCA 101-1983\") consolidated. \\\n",
    "The kernel of the structure specification language standards consists of regexes. \\\n",
    "Its use is evident in the DTD element group syntax.\n",
    "\n",
    "Starting in 1997, Philip Hazel developed PCRE (Perl Compatible Regular Expressions), \\\n",
    "which attempts to closely mimic Perl's regex functionality and is used by many modern tools including \\\n",
    "PHP and Apache HTTP Server.\n",
    "\n",
    "Today, regexes are widely supported in programming languages, text processing programs \\\n",
    "(particularly lexers), advanced text editors, and some other programs. \\\n",
    "Regex support is part of the standard library of many programming languages, including Java and Python, \\\n",
    "and is built into the syntax of others, including Perl and ECMAScript. Implementations of regex functionality \\\n",
    "is often called a regex engine, and a number of libraries are available for reuse. \\\n",
    "In the late 2010s, several companies started to offer hardware, FPGA[17], GPU[18] \\\n",
    "implementations of PCRE compatible regex engines that are faster compared to CPU implementations.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92038c9d637e074af546909457416ea7",
     "grade": false,
     "grade_id": "years_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Introduction / Years [10 Points]\n",
    "\n",
    "Familiarize yourself with the concept of regular expressions in general and the [Python syntax](https://docs.python.org/3.8/library/re.html) in particular.\n",
    "\n",
    "Create a regular expression to extract all years (e.g. 1983) from the text above. Assign your regular expression to the variable ```year``` below. Store all matches in the order they appear in the text as a list in the variable ```matches```.\n",
    "\n",
    "*Hint:* All years should be in this or the previous millenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b22c73a3925be9eba3811db645ffa28",
     "grade": false,
     "grade_id": "years",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years Found: ['1951', '1968', '1970', '1992', '1980', '1986', '1960', '1980', '1983', '1997', '2010']\n",
      "Number of matches: 11\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Tokenizing the text\n",
    "text_lines = list(text.split(\" \"))\n",
    "\n",
    "# Regular expression\n",
    "year = re.compile(r'[1-2][0-9][0-9][0-9]')\n",
    "\n",
    "# Obtaining matches\n",
    "match_objects = [re.search(year, i) for i in text_lines if re.search(year, i) is not None]\n",
    "matches = [re.findall(year, i) for i in text_lines if re.search(year, i) is not None]\n",
    "\n",
    "print(\"Years Found:\", sum(matches, []))\n",
    "print(\"Number of matches:\", len(sum(matches, [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dccf6d9ef5821440dae9d20c9f458272",
     "grade": true,
     "grade_id": "test_years",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd1f2613bd7f6b8281efa1db2a36c012",
     "grade": false,
     "grade_id": "footnotes_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Footnotes [10 Points]\n",
    "\n",
    "Find all footnotes or citation keys in the text. An example would be [17] or [12]. Assign your regular expression to the variable ```footnote```. Store all matches in the order they appear in the text as a list in the variable ```matches```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f74331e961c1f3226af1e293e979fa8",
     "grade": false,
     "grade_id": "footnotes",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citations Found: ['[4]', '[5]', '[6]', '[7]', '[6]', '[8]', '[9]', '[10]', '[11]', '[12]', '[7]', '[10]', '[13]', '[14]', '[15]', '[16]', '[17]', '[18]']\n",
      "Number of matches: 18\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Tokenizing the text\n",
    "text_list = list(text.split(\" \"))\n",
    "\n",
    "# Regular expression\n",
    "footnote = re.compile(r'\\[.*?\\]')\n",
    "\n",
    "# Obtaining matches\n",
    "match_objects = [re.search(footnote, i) for i in text_list if re.search(footnote, i) is not None]\n",
    "matches = [re.findall(footnote, i) for i in text_list if re.search(footnote, i) is not None]\n",
    "\n",
    "print(\"Citations Found:\", sum(matches, []))\n",
    "print(\"Number of matches:\", len(sum(matches, [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4694af827930e6294d78da8c74c645a",
     "grade": true,
     "grade_id": "test_footnotes",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89f3da3a733b88a19c4b04a3abc16874",
     "grade": false,
     "grade_id": "words_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Words [10 Points]\n",
    "\n",
    "Extract all words from the text. Ignore numerals and words in all capital letters. Assign your regular expression to the variable ```words```. Store all matches in the order they appear in the text as a list in the variable ```matches```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e96283f341295ed7f26b616fcb5e1c5b",
     "grade": false,
     "grade_id": "words",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words: ['Regular', 'expressions', 'originated', 'in', 'when', 'mathematician', 'Stephen', 'Cole', 'Kleene', 'described']\n",
      "Number of matches: 576\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regular expression\n",
    "words = re.compile(r\"\\b[A-Za-z][a-z]*(['-][a-z]+)?\\b\")\n",
    "# ALTERNATE REGEX: words = re.compile(r'^[a-zA-Z][a-z]+')\n",
    "\n",
    "# Obtaining matches\n",
    "matches = []\n",
    "list_words = re.finditer(words, text)\n",
    "for match in list_words:\n",
    "    matches.append(match[0])\n",
    "\n",
    "\n",
    "# Print the first 10 matches\n",
    "print(\"First 10 words:\", matches[:10])\n",
    "print(\"Number of matches:\", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04a02c6d65b51208688a7bd1d34f4e74",
     "grade": true,
     "grade_id": "test_words",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23922de4f7a0844d03eb4dba19fe15d8",
     "grade": false,
     "grade_id": "sentences_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Sentences [10 Points]\n",
    "\n",
    "Extract all the sentences from the text. Write a regular expression and store it in the variable ```sentence``` to identify all sentences. Do not worry if your regular expression can not handle abbreviations.\n",
    "Store all sentences in the order they appear in the text as a list in the variable ```sentences```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "812f6301666b7e54ed8b68975d313898",
     "grade": true,
     "grade_id": "sentences",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 sentences:\n",
      "['\\nRegular expressions originated in 1951, when mathematician Stephen Cole Kleene described regular languages using his mathematical notation called regular events.', '[4][5] These arose in theoretical computer science, in the subfields of automata theory (models of computation) and the description and classification of formal languages.', ' Other early implementations of pattern matching include the SNOBOL language, which did not use regular expressions, but instead its own pattern matching constructs.', '\\n\\nRegular expressions entered popular use from 1968 in two uses: pattern matching in a text editor[6] and lexical analysis in a compiler.', \"[7] Among the first appearances of regular expressions in program form was when Ken Thompson built Kleene's notation into the editor QED as a means to match patterns in text files.\", '[6][8][9][10] For speed, Thompson implemented regular expression matching by just-in-time compilation (JIT) to IBM 7094 code on the Compatible Time-Sharing System, an important early example of JIT compilation.', '[11] He later added this capability to the Unix editor ed, which eventually led to the popular search tool grep\\'s use of regular expressions (\"grep\" is a word derived from the command for regular expression searching in the ed editor: g/re/p meaning \"Global search for Regular Expression and Print matching lines\"[12]).', ' Around the same time when Thompson developed QED, a group of researchers including Douglas T.', ' Ross implemented a tool based on regular expressions that is used for lexical analysis in compiler design.', '[7]\\n\\nMany variations of these original forms of regular expressions were used in Unix[10] programs at Bell Labs in the 1970s, including vi, lex, sed, AWK, and expr, and in other programs such as Emacs.']\n",
      "\n",
      "Number of matches: 28\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "# Regular Expression\n",
    "sentence = re.compile(r'[A-Z]?.*?[.!?]', re.MULTILINE | re.DOTALL)\n",
    "\n",
    "# Obtaining matches \n",
    "sentences = re.findall(sentence, text) \n",
    "\n",
    "print(\"First 10 sentences:\")\n",
    "print(sentences[:10])\n",
    "print()\n",
    "print(\"Number of matches:\", len(sentences))\n",
    "\n",
    "# REFERENCES\n",
    "# 1. https://stackoverflow.com/questions/3549075/regex-to-find-all-sentences-of-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceba8c47fff516c03684d975820673c2",
     "grade": false,
     "grade_id": "tokenize_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Simple Tokenization [20 Points]\n",
    "\n",
    "Write a function ```tokenize``` that takes a document, processes it and outputs a list of lists of words.\n",
    "\n",
    "Each inner list should represent one sentence with the items being the words (these can be all caps and contain digits). \n",
    "\n",
    "*Example:*\n",
    "\n",
    "```\n",
    "text = \"I have a cat. I have a dog. I HAvE 5 chicken.\"\n",
    "\n",
    "output = tokenize(text)\n",
    "\n",
    "print(output) # -> [['I', 'have', 'a', 'cat'], \n",
    "                    ['I', 'have', 'a', 'dog'], \n",
    "                    ['I', 'HAvE', '5', 'chicken']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aabc53c63bc871a02faf94535604be2c",
     "grade": true,
     "grade_id": "tokenize",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text 1:\n",
      "[['I', 'have', 'a', 'cat'], ['I', 'have', 'a', 'dog'], ['I', 'HAvE', '5', 'chicken']]\n",
      "\n",
      "Tokenized text 2:\n",
      "[['Regular', 'expressions', 'originated', 'in', '1951', 'when', 'mathematician', 'Stephen', 'Cole', 'Kleene', 'described', 'regular', 'languages', 'using', 'his', 'mathematical', 'notation', 'called', 'regular', 'events'], ['4', '5', 'These', 'arose', 'in', 'theoretical', 'computer', 'science', 'in', 'the', 'subfields', 'of', 'automata', 'theory', 'models', 'of', 'computation', 'and', 'the', 'description', 'and', 'classification', 'of', 'formal', 'languages'], ['Other', 'early', 'implementations', 'of', 'pattern', 'matching', 'include', 'the', 'SNOBOL', 'language', 'which', 'did', 'not', 'use', 'regular', 'expressions', 'but', 'instead', 'its', 'own', 'pattern', 'matching', 'constructs'], ['Regular', 'expressions', 'entered', 'popular', 'use', 'from', '1968', 'in', 'two', 'uses', 'pattern', 'matching', 'in', 'a', 'text', 'editor', '6', 'and', 'lexical', 'analysis', 'in', 'a', 'compiler'], ['7', 'Among', 'the', 'first', 'appearances', 'of', 'regular', 'expressions', 'in', 'program', 'form', 'was', 'when', 'Ken', 'Thompson', 'built', 'Kleene', 's', 'notation', 'into', 'the', 'editor', 'QED', 'as', 'a', 'means', 'to', 'match', 'patterns', 'in', 'text', 'files'], ['6', '8', '9', '10', 'For', 'speed', 'Thompson', 'implemented', 'regular', 'expression', 'matching', 'by', 'just', 'in', 'time', 'compilation', 'JIT', 'to', 'IBM', '7094', 'code', 'on', 'the', 'Compatible', 'Time', 'Sharing', 'System', 'an', 'important', 'early', 'example', 'of', 'JIT', 'compilation'], ['11', 'He', 'later', 'added', 'this', 'capability', 'to', 'the', 'Unix', 'editor', 'ed', 'which', 'eventually', 'led', 'to', 'the', 'popular', 'search', 'tool', 'grep', 's', 'use', 'of', 'regular', 'expressions', 'grep', 'is', 'a', 'word', 'derived', 'from', 'the', 'command', 'for', 'regular', 'expression', 'searching', 'in', 'the', 'ed', 'editor', 'g', 're', 'p', 'meaning', 'Global', 'search', 'for', 'Regular', 'Expression', 'and', 'Print', 'matching', 'lines', '12'], ['Around', 'the', 'same', 'time', 'when', 'Thompson', 'developed', 'QED', 'a', 'group', 'of', 'researchers', 'including', 'Douglas', 'T'], ['Ross', 'implemented', 'a', 'tool', 'based', 'on', 'regular', 'expressions', 'that', 'is', 'used', 'for', 'lexical', 'analysis', 'in', 'compiler', 'design'], ['7', 'Many', 'variations', 'of', 'these', 'original', 'forms', 'of', 'regular', 'expressions', 'were', 'used', 'in', 'Unix', '10', 'programs', 'at', 'Bell', 'Labs', 'in', 'the', '1970s', 'including', 'vi', 'lex', 'sed', 'AWK', 'and', 'expr', 'and', 'in', 'other', 'programs', 'such', 'as', 'Emacs'], ['Regexes', 'were', 'subsequently', 'adopted', 'by', 'a', 'wide', 'range', 'of', 'programs', 'with', 'these', 'early', 'forms', 'standardized', 'in', 'the', 'POSIX'], ['2', 'standard', 'in', '1992'], ['In', 'the', '1980s', 'the', 'more', 'complicated', 'regexes', 'arose', 'in', 'Perl', 'which', 'originally', 'derived', 'from', 'a', 'regex', 'library', 'written', 'by', 'Henry', 'Spencer', '1986', 'who', 'later', 'wrote', 'an', 'implementation', 'of', 'Advanced', 'Regular', 'Expressions', 'for', 'Tcl'], ['13', 'The', 'Tcl', 'library', 'is', 'a', 'hybrid', 'NFA', 'DFA', 'implementation', 'with', 'improved', 'performance', 'characteristics'], ['Software', 'projects', 'that', 'have', 'adopted', 'Spencer', 's', 'Tcl', 'regular', 'expression', 'implementation', 'include', 'PostgreSQL'], ['14', 'Perl', 'later', 'expanded', 'on', 'Spencer', 's', 'original', 'library', 'to', 'add', 'many', 'new', 'features'], ['15', 'Part', 'of', 'the', 'effort', 'in', 'the', 'design', 'of', 'Raku', 'is', 'to', 'improve', 'Perl', 's', 'regex', 'integration', 'and', 'to', 'increase', 'their', 'scope', 'and', 'capabilities', 'to', 'allow', 'the', 'definition', 'of', 'parsing', 'expression', 'grammars'], ['16', 'The', 'result', 'is', 'a', 'mini', 'language', 'called', 'Raku', 'rules', 'which', 'are', 'used', 'to', 'define', 'Raku', 'grammar', 'as', 'well', 'as', 'provide', 'a', 'tool', 'to', 'programmers', 'in', 'the', 'language'], ['These', 'rules', 'maintain', 'existing', 'features', 'of', 'Perl', '5'], ['x', 'regexes', 'but', 'also', 'allow', 'BNF', 'style', 'definition', 'of', 'a', 'recursive', 'descent', 'parser', 'via', 'sub', 'rules'], ['The', 'use', 'of', 'regexes', 'in', 'structured', 'information', 'standards', 'for', 'document', 'and', 'database', 'modeling', 'started', 'in', 'the', '1960s', 'and', 'expanded', 'in', 'the', '1980s', 'when', 'industry', 'standards', 'like', 'ISO', 'SGML', 'precursored', 'by', 'ANSI', 'GCA', '101', '1983', 'consolidated'], ['The', 'kernel', 'of', 'the', 'structure', 'specification', 'language', 'standards', 'consists', 'of', 'regexes'], ['Its', 'use', 'is', 'evident', 'in', 'the', 'DTD', 'element', 'group', 'syntax'], ['Starting', 'in', '1997', 'Philip', 'Hazel', 'developed', 'PCRE', 'Perl', 'Compatible', 'Regular', 'Expressions', 'which', 'attempts', 'to', 'closely', 'mimic', 'Perl', 's', 'regex', 'functionality', 'and', 'is', 'used', 'by', 'many', 'modern', 'tools', 'including', 'PHP', 'and', 'Apache', 'HTTP', 'Server'], ['Today', 'regexes', 'are', 'widely', 'supported', 'in', 'programming', 'languages', 'text', 'processing', 'programs', 'particularly', 'lexers', 'advanced', 'text', 'editors', 'and', 'some', 'other', 'programs'], ['Regex', 'support', 'is', 'part', 'of', 'the', 'standard', 'library', 'of', 'many', 'programming', 'languages', 'including', 'Java', 'and', 'Python', 'and', 'is', 'built', 'into', 'the', 'syntax', 'of', 'others', 'including', 'Perl', 'and', 'ECMAScript'], ['Implementations', 'of', 'regex', 'functionality', 'is', 'often', 'called', 'a', 'regex', 'engine', 'and', 'a', 'number', 'of', 'libraries', 'are', 'available', 'for', 'reuse'], ['In', 'the', 'late', '2010s', 'several', 'companies', 'started', 'to', 'offer', 'hardware', 'FPGA', '17', 'GPU', '18', 'implementations', 'of', 'PCRE', 'compatible', 'regex', 'engines', 'that', 'are', 'faster', 'compared', 'to', 'CPU', 'implementations']]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def tokenize(text: str) -> List[List[str]]:\n",
    "    \n",
    "    # Finding sentences\n",
    "    sentence = re.compile(r'[A-Z]?.*?[\\.!?]', re.MULTILINE | re.DOTALL)\n",
    "    sentences = re.findall(sentence, text) \n",
    "    \n",
    "    # Finding words\n",
    "    words = re.compile(r'[\\w]+')\n",
    "    matches = [re.findall(words, i) for i in sentences if re.search(words, i) is not None]\n",
    "    \n",
    "    return matches\n",
    "\n",
    "print('Tokenized text 1:')\n",
    "print(tokenize(\"I have a cat. I have a dog. I HAvE 5 chicken.\"))\n",
    "print()\n",
    "print('Tokenized text 2:')\n",
    "print((tokenize(text)))\n",
    "# print(\"Number of matches:\", len(tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "208b0e4554b677209d523d3a9ab82118",
     "grade": false,
     "grade_id": "pipeline_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Text Processing Pipeline [40 Points]\n",
    "\n",
    "Now we want to compile everything we did so far together into a text processing pipeline for Wikipedia articles.\n",
    "\n",
    "For this you are provided with the following three classes:\n",
    "\n",
    "- ```Pipeline```: Class that consists of processing stages, to which new stages can be added\n",
    "- ```RemoveCitationKeys```: Class that removes all citation keys of the form [1], [23], ... from the text\n",
    "- ```Tokenize```: Class that splits the text into sentences and words (results in a list of lists of strings)\n",
    "\n",
    "Each stage gets the result of the previous stage as the input and produces a result for the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "173032305ac38c3c379591e842353d32",
     "grade": true,
     "grade_id": "pipeline",
     "locked": false,
     "points": 40,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text:\n",
      "[['Regular', 'expressions', 'originated', 'in', '1951', 'when', 'mathematician', 'Stephen', 'Cole', 'Kleene', 'described', 'regular', 'languages', 'using', 'his', 'mathematical', 'notation', 'called', 'regular', 'events'], ['These', 'arose', 'in', 'theoretical', 'computer', 'science', 'in', 'the', 'subfields', 'of', 'automata', 'theory', 'models', 'of', 'computation', 'and', 'the', 'description', 'and', 'classification', 'of', 'formal', 'languages'], ['Other', 'early', 'implementations', 'of', 'pattern', 'matching', 'include', 'the', 'SNOBOL', 'language', 'which', 'did', 'not', 'use', 'regular', 'expressions', 'but', 'instead', 'its', 'own', 'pattern', 'matching', 'constructs'], ['Regular', 'expressions', 'entered', 'popular', 'use', 'from', '1968', 'in', 'two', 'uses', 'pattern', 'matching', 'in', 'a', 'text', 'editor', 'and', 'lexical', 'analysis', 'in', 'a', 'compiler'], ['Among', 'the', 'first', 'appearances', 'of', 'regular', 'expressions', 'in', 'program', 'form', 'was', 'when', 'Ken', 'Thompson', 'built', 'Kleene', 's', 'notation', 'into', 'the', 'editor', 'QED', 'as', 'a', 'means', 'to', 'match', 'patterns', 'in', 'text', 'files'], ['For', 'speed', 'Thompson', 'implemented', 'regular', 'expression', 'matching', 'by', 'just', 'in', 'time', 'compilation', 'JIT', 'to', 'IBM', '7094', 'code', 'on', 'the', 'Compatible', 'Time', 'Sharing', 'System', 'an', 'important', 'early', 'example', 'of', 'JIT', 'compilation'], ['He', 'later', 'added', 'this', 'capability', 'to', 'the', 'Unix', 'editor', 'ed', 'which', 'eventually', 'led', 'to', 'the', 'popular', 'search', 'tool', 'grep', 's', 'use', 'of', 'regular', 'expressions', 'grep', 'is', 'a', 'word', 'derived', 'from', 'the', 'command', 'for', 'regular', 'expression', 'searching', 'in', 'the', 'ed', 'editor', 'g', 're', 'p', 'meaning', 'Global', 'search', 'for', 'Regular', 'Expression', 'and', 'Print', 'matching', 'lines'], ['Around', 'the', 'same', 'time', 'when', 'Thompson', 'developed', 'QED', 'a', 'group', 'of', 'researchers', 'including', 'Douglas', 'T'], ['Ross', 'implemented', 'a', 'tool', 'based', 'on', 'regular', 'expressions', 'that', 'is', 'used', 'for', 'lexical', 'analysis', 'in', 'compiler', 'design'], ['Many', 'variations', 'of', 'these', 'original', 'forms', 'of', 'regular', 'expressions', 'were', 'used', 'in', 'Unix', 'programs', 'at', 'Bell', 'Labs', 'in', 'the', '1970s', 'including', 'vi', 'lex', 'sed', 'AWK', 'and', 'expr', 'and', 'in', 'other', 'programs', 'such', 'as', 'Emacs'], ['Regexes', 'were', 'subsequently', 'adopted', 'by', 'a', 'wide', 'range', 'of', 'programs', 'with', 'these', 'early', 'forms', 'standardized', 'in', 'the', 'POSIX'], ['2', 'standard', 'in', '1992'], ['In', 'the', '1980s', 'the', 'more', 'complicated', 'regexes', 'arose', 'in', 'Perl', 'which', 'originally', 'derived', 'from', 'a', 'regex', 'library', 'written', 'by', 'Henry', 'Spencer', '1986', 'who', 'later', 'wrote', 'an', 'implementation', 'of', 'Advanced', 'Regular', 'Expressions', 'for', 'Tcl'], ['The', 'Tcl', 'library', 'is', 'a', 'hybrid', 'NFA', 'DFA', 'implementation', 'with', 'improved', 'performance', 'characteristics'], ['Software', 'projects', 'that', 'have', 'adopted', 'Spencer', 's', 'Tcl', 'regular', 'expression', 'implementation', 'include', 'PostgreSQL'], ['Perl', 'later', 'expanded', 'on', 'Spencer', 's', 'original', 'library', 'to', 'add', 'many', 'new', 'features'], ['Part', 'of', 'the', 'effort', 'in', 'the', 'design', 'of', 'Raku', 'is', 'to', 'improve', 'Perl', 's', 'regex', 'integration', 'and', 'to', 'increase', 'their', 'scope', 'and', 'capabilities', 'to', 'allow', 'the', 'definition', 'of', 'parsing', 'expression', 'grammars'], ['The', 'result', 'is', 'a', 'mini', 'language', 'called', 'Raku', 'rules', 'which', 'are', 'used', 'to', 'define', 'Raku', 'grammar', 'as', 'well', 'as', 'provide', 'a', 'tool', 'to', 'programmers', 'in', 'the', 'language'], ['These', 'rules', 'maintain', 'existing', 'features', 'of', 'Perl', '5'], ['x', 'regexes', 'but', 'also', 'allow', 'BNF', 'style', 'definition', 'of', 'a', 'recursive', 'descent', 'parser', 'via', 'sub', 'rules'], ['The', 'use', 'of', 'regexes', 'in', 'structured', 'information', 'standards', 'for', 'document', 'and', 'database', 'modeling', 'started', 'in', 'the', '1960s', 'and', 'expanded', 'in', 'the', '1980s', 'when', 'industry', 'standards', 'like', 'ISO', 'SGML', 'precursored', 'by', 'ANSI', 'GCA', '101', '1983', 'consolidated'], ['The', 'kernel', 'of', 'the', 'structure', 'specification', 'language', 'standards', 'consists', 'of', 'regexes'], ['Its', 'use', 'is', 'evident', 'in', 'the', 'DTD', 'element', 'group', 'syntax'], ['Starting', 'in', '1997', 'Philip', 'Hazel', 'developed', 'PCRE', 'Perl', 'Compatible', 'Regular', 'Expressions', 'which', 'attempts', 'to', 'closely', 'mimic', 'Perl', 's', 'regex', 'functionality', 'and', 'is', 'used', 'by', 'many', 'modern', 'tools', 'including', 'PHP', 'and', 'Apache', 'HTTP', 'Server'], ['Today', 'regexes', 'are', 'widely', 'supported', 'in', 'programming', 'languages', 'text', 'processing', 'programs', 'particularly', 'lexers', 'advanced', 'text', 'editors', 'and', 'some', 'other', 'programs'], ['Regex', 'support', 'is', 'part', 'of', 'the', 'standard', 'library', 'of', 'many', 'programming', 'languages', 'including', 'Java', 'and', 'Python', 'and', 'is', 'built', 'into', 'the', 'syntax', 'of', 'others', 'including', 'Perl', 'and', 'ECMAScript'], ['Implementations', 'of', 'regex', 'functionality', 'is', 'often', 'called', 'a', 'regex', 'engine', 'and', 'a', 'number', 'of', 'libraries', 'are', 'available', 'for', 'reuse'], ['In', 'the', 'late', '2010s', 'several', 'companies', 'started', 'to', 'offer', 'hardware', 'FPGA', 'GPU', 'implementations', 'of', 'PCRE', 'compatible', 'regex', 'engines', 'that', 'are', 'faster', 'compared', 'to', 'CPU', 'implementations']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stages = []\n",
    "    \n",
    "    def add_stage(self, stage):\n",
    "        '''\n",
    "        Add a stage to the pipeline\n",
    "        \n",
    "        Arguments:\n",
    "            stage -- Instance of a stage with the method process\n",
    "        '''\n",
    "        self.stages.append(stage)\n",
    "        \n",
    "    def process(self, text):\n",
    "        '''\n",
    "        Process a text by applying the registered stages\n",
    "        \n",
    "        Arguments:\n",
    "            text           -- string of a body of text\n",
    "        Returns:\n",
    "            processed text -- Text processed by the registered stages\n",
    "        '''\n",
    "        for stage in self.stages:\n",
    "            text = stage.process(text)\n",
    "        return text\n",
    "    \n",
    "class RemoveCitationKeys:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.footnote = re.compile(r'\\[.+?\\]')\n",
    "    \n",
    "    def process(self, text):\n",
    "        text_strip = re.sub(self.footnote, \"\", text)\n",
    "        return text_strip\n",
    "            \n",
    "class Tokenize:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sentence = re.compile(r\"\\s*?.*?[.?!]\", re.MULTILINE | re.DOTALL)\n",
    "        self.words = re.compile(r'[\\w]+')\n",
    "    \n",
    "    def process(self, text):\n",
    "        sentences = re.findall(self.sentence, text) \n",
    "        matches = [re.findall(self.words, i) for i in sentences if re.search(self.words, i) is not None]\n",
    "            \n",
    "        return matches\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_stage(RemoveCitationKeys())\n",
    "pipeline.add_stage(Tokenize())\n",
    "print(\"Tokenized text:\")\n",
    "print(pipeline.process(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
