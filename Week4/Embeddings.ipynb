{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powered-slide",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbassignment": {
     "type": "header"
    },
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4fd6b7550484ed709bca7da18028042",
     "grade": false,
     "grade_id": "template_886979f3_0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Wednesday at 15:00</b>.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to two people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controversial-biography",
   "metadata": {
    "nbassignment": {
     "type": "group_info"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the UID (i.e. student2s) of each team member into the variables. \n",
    "If you work alone please leave the second variable empty, or extend the list if necessary.\n",
    "'''\n",
    "member1 = 'Syed Mushrraf Ali (sali2s, 9040658)'\n",
    "member2 = 'Shalaka Satheesh (ssathe2s, 9040760)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-relationship",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e864a3294593c42ef120adc5cc716591",
     "grade": false,
     "grade_id": "VectorSimilarity_AVectorSimilarity_BVectorSimilarity_CVectorSimilarity_DVectorSimilarity_EVectorSimilarity_FVectorSimilarity_G_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Word2Vec and FastText Embeddings\n",
    "\n",
    "In this assignment we will work on Word2Vec embeddings and FastText embeddings.\n",
    "\n",
    "I prepared three dictionaries for you:\n",
    "\n",
    "- ```word2vec_yelp_vectors.pkl```: A dictionary with 300 dimensional word2vec embeddings trained on the Google News Corpus, contains only words that are present in our Yelp reviews (key is the word, value is the embedding)\n",
    "- ```fasttext_yelp_vectors.pkl```: A dictionary with 300 dimensional FastText embeddings trained on the English version of Wikipedia, contains only words that are present in our Yelp reviews (key is the word, value is the embedding)\n",
    "- ```tfidf_yelp_vectors.pkl```: A dictionary with 400 dimensional TfIdf embeddings trained on the Yelp training dataset from last assignment (key is the word, value is the embedding)\n",
    "\n",
    "In the next cell we load those into the dictionaries ```w2v_vectors```, ```ft_vectors``` and ```tfidf_vectors```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sensitive-following",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31ab09bf8d67973e20d956cbbfa995b0",
     "grade": false,
     "grade_id": "VectorSimilarity_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/srv/shares/NLP/word2vec_yelp_vectors.pkl', 'rb') as f:\n",
    "    w2v_vectors = pickle.loads(f.read())\n",
    "    \n",
    "with open('/srv/shares/NLP/fasttext_yelp_vectors.pkl', 'rb') as f:\n",
    "    ft_vectors = pickle.loads(f.read())\n",
    "    \n",
    "with open('/srv/shares/NLP/tfidf_yelp_vectors.pkl', 'rb') as f:\n",
    "    tfidf_vectors = pickle.loads(f.read())\n",
    "    \n",
    "with open('/srv/shares/NLP/reviews_train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "with open('/srv/shares/NLP/reviews_test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "reviews = train + test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-vermont",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f6c0346e0db0e944394ac0a918f72e9",
     "grade": false,
     "grade_id": "VectorSimilarity_A_Description1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Creating a vector model with helper functions [30 points]\n",
    "\n",
    "In the next cell we have the class ```VectorModel``` with the methods:\n",
    "\n",
    "- ```vector_size```: Returns the vector size of the model\n",
    "- ```embed```: Returns the embedding for a word. Returns None if there is no embedding present for the word\n",
    "- ```cosine_similarity```: Calculates the cosine similarity between two vectors\n",
    "- ```most_similar```: Given a word returns the ```top_n``` most similar words from the model, together with the similarity value, **sorted by similarity (descending)**.\n",
    "- ```most_similar_vec```: Given a vector returns the ```top_n``` most similar words from the model, together with the similarity value, **sorted by similarity (descending)**.\n",
    "\n",
    "Your task is to complete these methods.\n",
    "\n",
    "Example output:\n",
    "```\n",
    "model = VectorModel(w2v_vectors)\n",
    "\n",
    "vector_good = model.embed('good')\n",
    "vector_tomato = model.embed('tomato')\n",
    "\n",
    "print(model.cosine_similarity(vector_good, vector_tomato)) # Prints: 0.05318105\n",
    "\n",
    "print(model.most_similar('tomato')) \n",
    "'''\n",
    "[('tomatoes', 0.8442263), \n",
    " ('lettuce', 0.70699364),\n",
    " ('strawberry', 0.6888598), \n",
    " ('strawberries', 0.68325955), \n",
    " ('potato', 0.67841727)]\n",
    "'''\n",
    "\n",
    "print(model.most_similar_vec(vector_good)) \n",
    "'''\n",
    "[('good', 1.0), \n",
    " ('great', 0.72915095), \n",
    " ('bad', 0.7190051), \n",
    " ('decent', 0.6837349), \n",
    " ('nice', 0.68360925)]\n",
    "'''\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "honest-little",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8dcf13a0fc232d23f818a43ce16c475",
     "grade": false,
     "grade_id": "VectorSimilarity_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "   \n",
    "class VectorModel:\n",
    "    \n",
    "    def __init__(self, vector_dict: Dict[str, np.ndarray]):\n",
    "         self.vector_dict = vector_dict\n",
    "        \n",
    "    def embed(self, word: str) -> np.ndarray:\n",
    "        try:\n",
    "            embedding = self.vector_dict[word]\n",
    "            return embedding\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def vector_size(self) -> int:\n",
    "        return np.size(list(self.vector_dict.values())[0])\n",
    "    \n",
    "    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1, ord=2) * np.linalg.norm(vec2, ord=2))\n",
    "        return similarity\n",
    "\n",
    "    def most_similar(self, word: str, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        cosines = {key: self.cosine_similarity(self.embed(word), neighbour) for key, neighbour in self.vector_dict.items()}\n",
    "        cosines_sorted = dict(sorted(cosines.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        most_similar = []\n",
    "        \n",
    "        for i in range(top_n):\n",
    "            most_similar.append((list(cosines_sorted.keys())[1], list(cosines_sorted.values())[1]))\n",
    "            del cosines_sorted[list(cosines_sorted.keys())[1]]\n",
    "        \n",
    "        return most_similar\n",
    "    \n",
    "    def most_similar_vec(self, vec: np.ndarray, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        cosines = {key: self.cosine_similarity(vec, neighbour) for key, neighbour in self.vector_dict.items()}\n",
    "        cosines_sorted = dict(sorted(cosines.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "        most_similar = []\n",
    "        \n",
    "        for i in range(top_n):\n",
    "            most_similar.append((list(cosines_sorted.keys())[0], list(cosines_sorted.values())[0]))\n",
    "            del cosines_sorted[list(cosines_sorted.keys())[0]]\n",
    "        \n",
    "        return most_similar\n",
    "    \n",
    "    def embedding_used(self) -> str:\n",
    "        '''\n",
    "        Function to return the name of the embedding used\n",
    "        '''\n",
    "        if self.vector_dict == w2v_vectors:\n",
    "            return 'word2vec'\n",
    "        elif self.vector_dict == ft_vectors:\n",
    "            return 'fasttext'\n",
    "        elif self.vector_dict == tfidf_vectors:\n",
    "            return 'TFIDF'\n",
    "        \n",
    "\n",
    "# model = VectorModel(w2v_vectors) \n",
    "# model.vector_size()\n",
    "\n",
    "# vector_good = model.embed('good')\n",
    "# vector_tomato = model.embed('tomato')\n",
    "\n",
    "# print(model.cosine_similarity(vector_good, vector_tomato)) \n",
    "# print(model.most_similar('tomato')) \n",
    "# print(model.most_similar_vec(vector_good)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unknown-fault",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b980304ecfda2264fb207439e3b3c925",
     "grade": true,
     "grade_id": "test_VectorSimilarity_A0",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-norway",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc8f7068215ca1ef592778c5a4b4162a",
     "grade": false,
     "grade_id": "VectorSimilarity_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity A) [10 points]\n",
    "\n",
    "We now want to find the most similar words for a given input word for each model (Word2Vec, FastText and TfIdf).\n",
    "\n",
    "Your input words are: ```['good', 'tomato', 'restaurant', 'beer', 'wonderful']```.\n",
    "\n",
    "For each model and input word print the top three most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improved-count",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddad901f2d646951490e9fcfe8edb004",
     "grade": true,
     "grade_id": "VectorSimilarity_B",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 words using word2vec are:\n",
      "\n",
      "For good: [('great', 0.72915095), ('bad', 0.7190051), ('decent', 0.6837349)]\n",
      "\n",
      "For tomato: [('tomatoes', 0.8442263), ('lettuce', 0.70699364), ('strawberry', 0.6888598)]\n",
      "\n",
      "For restaurant: [('restaurants', 0.7722894), ('diner', 0.7280216), ('steakhouse', 0.7269855)]\n",
      "\n",
      "For beer: [('beers', 0.8409688), ('drinks', 0.66893125), ('ale', 0.63828725)]\n",
      "\n",
      "For wonderful: [('fantastic', 0.8047919), ('great', 0.76478696), ('fabulous', 0.7614761)]\n",
      "\n",
      "For dinner: [('dinners', 0.7902064), ('brunch', 0.7900513), ('breakfast', 0.7007028)]\n",
      "\n",
      "================================================================================\n",
      "Top 3 words using fasttext are:\n",
      "\n",
      "For good: [('excellent', 0.7223856825801253), ('decent', 0.7202461451724537), ('bad', 0.6704173041669614)]\n",
      "\n",
      "For tomato: [('eggplant', 0.7518509618329048), ('spinach', 0.7422800959168397), ('onions', 0.7328857483500282)]\n",
      "\n",
      "For restaurant: [('restaurants', 0.8384667264823358), ('bistro', 0.7845601578005464), ('bakery', 0.7155727705943096)]\n",
      "\n",
      "For beer: [('beers', 0.7944971406865431), ('brewed', 0.7929903321082488), ('brewery', 0.7520785637582764)]\n",
      "\n",
      "For wonderful: [('lovely', 0.6808215868395575), ('fascinating', 0.6745727685452474), ('amazing', 0.6457084279396067)]\n",
      "\n",
      "For dinner: [('dinners', 0.8463012295986414), ('brunch', 0.709906334988423), ('meal', 0.6719537670739056)]\n",
      "\n",
      "================================================================================\n",
      "Top 3 words using TFIDF are:\n",
      "\n",
      "For good: [('the', 0.6199071144484399), ('a', 0.6170194328254505), ('and', 0.6121212998064653)]\n",
      "\n",
      "For tomato: [('provolone', 0.7071067811865476), ('ceasar', 0.7071067811865475), ('cheesesteak', 0.7071067811865475)]\n",
      "\n",
      "For restaurant: [('held', 0.46881558553353014), ('patrons', 0.4617834076808424), ('we', 0.4308073684733683)]\n",
      "\n",
      "For beer: [('tap', 0.6326077388711024), ('beers', 0.5132564299212761), ('blowingly', 0.4746774221449982)]\n",
      "\n",
      "For wonderful: [('truffle', 0.6264995084522798), ('accident', 0.5432509277196604), ('equally', 0.5432509277196604)]\n",
      "\n",
      "For dinner: [('slaw', 0.4842078303350308), ('timely', 0.4038564890706335), ('plates', 0.3828945646253459)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_words = ['good', 'tomato', 'restaurant', 'beer', 'wonderful', 'dinner']\n",
    "\n",
    "def get_similar_words(model: VectorModel, input_words: List, top_n: int):\n",
    "    print(\"Top\", top_n, \"words using\", model.embedding_used(), \"are:\")\n",
    "    print()\n",
    "    for word in input_words:\n",
    "        print('For', word+':', model.most_similar(word, top_n))\n",
    "        print()\n",
    "    \n",
    "model_1 = VectorModel(w2v_vectors)\n",
    "model_2 = VectorModel(ft_vectors)\n",
    "model_3 = VectorModel(tfidf_vectors)\n",
    "\n",
    "get_similar_words(model_1, input_words, 3)\n",
    "print(\"=\"*80)\n",
    "get_similar_words(model_2, input_words, 3)\n",
    "print(\"=\"*80)\n",
    "get_similar_words(model_3, input_words, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-ultimate",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea42db99d9e4f8eceab5247a1e742d2f",
     "grade": false,
     "grade_id": "VectorSimilarity_C_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity B) [10 points]\n",
    "\n",
    "Comment on the output from the previous task. Let us look at the output for the word ```wonderful```. How do the models differ for this word? Can you reason why the TfIdf model shows so different results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-aviation",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "560f030fbc0ebf9c093d42f0af3d37c5",
     "grade": true,
     "grade_id": "VectorSimilarity_C",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "fasttext and word2vec perform similarly. fasttext is just an extension of word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-answer",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0577bd13c5771b100b73b4d013022259",
     "grade": false,
     "grade_id": "VectorSimilarity_D_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity C) [10 points]\n",
    "\n",
    "Instead of just finding the most similar word to a single word, we can also find the most similar word given a list of positive and negative words.\n",
    "\n",
    "For this we just sum up the positive and negative words into a single vector by calculating a weighted mean. For this we multiply each positive word with a factor of $+1$ and each negative word with a factor of $-1$. Then we get the most similar words to that vector.\n",
    "\n",
    "You are given the following examples:\n",
    "\n",
    "```\n",
    "inputs = [\n",
    "    {\n",
    "        'positive': ['good', 'wonderful'],\n",
    "        'negative': ['bad']\n",
    "    },\n",
    "    {\n",
    "        'positive': ['tomato', 'lettuce'],\n",
    "        'negative': ['strawberry', 'salad']\n",
    "    }    \n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advanced-shelf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c11d7cc8e7d41bae0b33df45bc3e5433",
     "grade": true,
     "grade_id": "VectorSimilarity_D",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words using word2vec are:\n",
      "\n",
      "For input 0: [('wonderful', 0.9038065), ('good', 0.86836797), ('great', 0.84323597), ('fantastic', 0.82130545), ('nice', 0.75373894)]\n",
      "\n",
      "For input 1: [('lettuce', 0.9304779), ('tomato', 0.9169305), ('tomatoes', 0.86696106), ('spinach', 0.7767467), ('broccoli', 0.7444947)]\n",
      "\n",
      "For input 2: [('chicken', 1.0), ('meat', 0.6799131), ('pork', 0.6541997), ('turkey', 0.62825197), ('shrimp', 0.6004993)]\n",
      "\n",
      "================================================================================\n",
      "Top 5 words using fasttext are:\n",
      "\n",
      "For input 0: [('wonderful', 0.901771055347115), ('good', 0.8790438652506161), ('excellent', 0.7178267179872835), ('decent', 0.665085234039435), ('lovely', 0.6603127457802108)]\n",
      "\n",
      "For input 1: [('lettuce', 0.9264417334730983), ('tomato', 0.9140209476178994), ('spinach', 0.7932212016028506), ('eggplant', 0.7772270057499169), ('onions', 0.7737536660800143)]\n",
      "\n",
      "For input 2: [('chicken', 0.8072479914245532), ('ceasar', 0.7750985549424643), ('beef', 0.6371025365057209), ('pork', 0.6124103144819651), ('hamburgers', 0.6056037232373332)]\n",
      "\n",
      "================================================================================\n",
      "Top 5 words using TFIDF are:\n",
      "\n",
      "For input 0: [('good', 0.8037979493602913), ('wonderful', 0.6457279945893748), ('a', 0.5589242950688014), ('and', 0.5550416610660979), ('the', 0.5502634351954051)]\n",
      "\n",
      "For input 1: [('lettuce', 0.7757201717623923), ('tomato', 0.6310770278824327), ('mayo', 0.46645274522525915), ('ceasar', 0.4462388458667201), ('cheesesteak', 0.4462388458667201)]\n",
      "\n",
      "For input 2: [('chicken', 0.9645551881849636), ('sauce', 0.3962011171668339), ('dumpling', 0.376011516357979), ('lotus', 0.376011516357979), ('food', 0.3589247374018773)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    {\n",
    "        'positive': ['good', 'wonderful'],\n",
    "        'negative': ['bad']\n",
    "    },\n",
    "    {\n",
    "        'positive': ['tomato', 'lettuce'],\n",
    "        'negative': ['strawberry', 'fruit']\n",
    "    },\n",
    "    {\n",
    "        'positive': ['ceasar', 'chicken'],\n",
    "        'negative': []\n",
    "    }    \n",
    "]\n",
    "\n",
    "def similarity_given_posneg(model, inputs: List, top_n: int) -> List:\n",
    "    inputs_vec_values = []\n",
    "    for posneg in inputs:\n",
    "        posneg_vec_values = {}\n",
    "        for key, words in posneg.items():\n",
    "            embeddings = []\n",
    "            for word in words:\n",
    "                embeddings.append(model.embed(word))\n",
    "            if key == 'positive':\n",
    "                posneg_vec_values[key] = embeddings\n",
    "            elif key == 'negative':\n",
    "                try:\n",
    "                    posneg_vec_values[key] = (embeddings * -1)\n",
    "                # ELEMENT WISE MULTIPLICATION ???\n",
    "#                 posneg_vec_values[key] = np.dot(embeddings, -1)\n",
    "                except:\n",
    "                    posneg_vec_values[key] = embeddings\n",
    "\n",
    "        inputs_vec_values.append(posneg_vec_values)\n",
    "    \n",
    "    summed_vec = []\n",
    "    for i, posneg in enumerate(inputs_vec_values):\n",
    "        summed_vec_inter = []\n",
    "        for key, words in posneg.items():\n",
    "            for word in words:\n",
    "                if word is not None:\n",
    "                    # if summed_vec_inter != []:\n",
    "                    try:\n",
    "                        summed_vec_inter = summed_vec_inter + word\n",
    "                    except:\n",
    "                        summed_vec_inter = word\n",
    "\n",
    "        summed_vec.append(summed_vec_inter)\n",
    "    \n",
    "    print(\"Top\", top_n, \"words using\", model.embedding_used(), \"are:\")\n",
    "    print()\n",
    "    for i, vector in enumerate(summed_vec):\n",
    "        print('For input', str(i)+':', model.most_similar_vec(vector, top_n))\n",
    "        print()\n",
    "\n",
    "\n",
    "model_1 = VectorModel(w2v_vectors)\n",
    "model_2 = VectorModel(ft_vectors)\n",
    "model_3 = VectorModel(tfidf_vectors)\n",
    "\n",
    "similarity_given_posneg(model_1, inputs, 5)\n",
    "print(\"=\"*80)\n",
    "similarity_given_posneg(model_2, inputs, 5)\n",
    "print(\"=\"*80)\n",
    "similarity_given_posneg(model_3, inputs, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-reset",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3aeb5076326c30f2b229061cc1d1dc58",
     "grade": false,
     "grade_id": "VectorSimilarity_E_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity D) [15 points]\n",
    "\n",
    "We can use our model to find out which word does not match given a list of words.\n",
    "\n",
    "For this we build the mean vector of all embeddings in the list.  \n",
    "Then we calculate the cosine similarity between the mean and all those vectors.\n",
    "\n",
    "The word that does not match is then the word with the lowest cosine similarity to the mean.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "model = VectorModel(w2v_vectors)\n",
    "doesnt_match(model, ['potato', 'tomato', 'beer']) # -> 'beer'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "featured-ballot",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb6cae44cef647800d07c81569889966",
     "grade": false,
     "grade_id": "VectorSimilarity_E",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vegetable\n"
     ]
    }
   ],
   "source": [
    "def doesnt_match(model, words):\n",
    "    \n",
    "    sum_vector = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            sum_vector = sum_vector + model.embed(word)\n",
    "        except:\n",
    "            sum_vector = model.embed(word)\n",
    "    mean_vector = sum_vector / len(words)\n",
    "\n",
    "    cosine_similarities = dict()\n",
    "    for word in words:\n",
    "        cosine_similarities[word] = model.cosine_similarity(mean_vector, model.embed(word))\n",
    "    \n",
    "    return list(dict(sorted(cosine_similarities.items(), key=lambda item: item[1], reverse=False)).items())[0][0]\n",
    "    \n",
    "model_tfidf = VectorModel(tfidf_vectors)\n",
    "print(doesnt_match(model_tfidf, ['vegetable', 'strawberry', 'tomato', 'lettuce']))\n",
    "\n",
    "# model = VectorModel(w2v_vectors)\n",
    "# print(doesnt_match(model, ['potato', 'tomato', 'beer']))\n",
    "# print(doesnt_match(model, ['vegetable', 'strawberry', 'tomato', 'lettuce']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "improved-conclusion",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98595e3698692eaedef7c03eac3b1da9",
     "grade": true,
     "grade_id": "test_VectorSimilarity_E0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-parent",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d141ef5c3f341644d3a756404c50be53",
     "grade": false,
     "grade_id": "VectorSimilarity_F_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Document Embeddings A) [15 points]\n",
    "\n",
    "Now we want to create document embeddings similar to the last assignment. For this you are given the function ```bagOfWords```. In the context of Word2Vec and FastText embeddings this is also called ```SOWE``` for sum of word embeddings.\n",
    "\n",
    "Take the yelp reviews (```reviews```) and create a dictionary containing the document id as a key and the document embedding as a value.\n",
    "\n",
    "Create the document embeddings from the Word2Vec, FastText and TfIdf embeddings.\n",
    "\n",
    "Store these in the variables ```ft_doc_embeddings```, ```w2v_doc_embeddings``` and ```tfidf_doc_embeddings```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "serial-forum",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cba6e7a1b4767cbd653d557dac01a02",
     "grade": false,
     "grade_id": "VectorSimilarity_F",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def bagOfWords(model: VectorModel, doc: List[str]) -> np.ndarray:\n",
    "    '''\n",
    "    Create a document embedding using the bag of words approach\n",
    "    \n",
    "    Args:\n",
    "        model     -- The embedding model to use\n",
    "        doc       -- A document as a list of tokens\n",
    "        \n",
    "    Returns:\n",
    "        embedding -- The embedding for the document as a single vector \n",
    "    '''\n",
    "    embeddings = [np.zeros(model.vector_size())]\n",
    "    n_tokens = 0\n",
    "    for token in doc:\n",
    "        embedding = model.embed(token)\n",
    "        if embedding is not None:\n",
    "            n_tokens += 1\n",
    "            embeddings.append(embedding)\n",
    "    if n_tokens > 0:\n",
    "        return sum(embeddings)/n_tokens\n",
    "    return sum(embeddings)\n",
    "\n",
    "\n",
    "ft_doc_embeddings = dict()\n",
    "w2v_doc_embeddings = dict()\n",
    "tfidf_doc_embeddings = dict()\n",
    "\n",
    "model_1 = VectorModel(w2v_vectors)\n",
    "model_2 = VectorModel(ft_vectors)\n",
    "model_3 = VectorModel(tfidf_vectors)\n",
    "\n",
    "# WHAT IS THE CATCH HERE???\n",
    "for doc in reviews:\n",
    "    w2v_doc_embeddings[doc['id']] = bagOfWords(model_1, doc['tokens'])\n",
    "    ft_doc_embeddings[doc['id']] = bagOfWords(model_2, doc['tokens'])\n",
    "    tfidf_doc_embeddings[doc['id']] = bagOfWords(model_3, doc['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "common-forwarding",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfbc55cfab28e60b281e7e06412310ff",
     "grade": true,
     "grade_id": "test_VectorSimilarity_F0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-slovak",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4113d00f19c775ea92044ce0442ad250",
     "grade": false,
     "grade_id": "VectorSimilarity_G_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Document Embeddings B) [10 points]\n",
    "\n",
    "Create a vector model from each of the document embedding dictionaries. Call these ```model_w2v_doc```, ```model_ft_doc``` and ```model_tfidf_doc```.\n",
    "\n",
    "Now find the most similar document (```top_n=1```) for document $438$ with each of these models. Print the text for each of the most similar reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "persistent-fairy",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71ccd4569a5255386cafbbbc1cc0b510",
     "grade": true,
     "grade_id": "VectorSimilarity_G",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source document:\n",
      "Absolutely ridiculously amazing! Chicken Tikka masala was perfect. Best I've ever had!\n",
      "======================================================================\n",
      "Similar document:\n",
      "I think I've been spoiled by eating delicious quesadillas quite frequently because the chicken quesadilla I ate was sub par.  It was greasy and the quality of chicken was not impressive. I gave an extra star because you can choose the fillings and those were fresh.\n"
     ]
    }
   ],
   "source": [
    "# First find the text for review 438\n",
    "def find_doc(doc_id, reviews):\n",
    "    for review in reviews:\n",
    "        if review['id'] == doc_id:\n",
    "            return review['text']\n",
    "    \n",
    "doc_id = 438\n",
    "\n",
    "# Print it\n",
    "print('Source document:')\n",
    "print(find_doc(doc_id, reviews))\n",
    "\n",
    "\n",
    "# Create the models\n",
    "model_w2v_doc =  VectorModel(w2v_doc_embeddings)\n",
    "model_ft_doc = VectorModel(ft_doc_embeddings)\n",
    "model_tfidf_doc = VectorModel(tfidf_doc_embeddings)\n",
    "\n",
    "model_w2v_doc.most_similar_vec(model_w2v_doc.vector_dict[438], 2)\n",
    "\n",
    "print('='*70)\n",
    "print('Similar document:')\n",
    "print(find_doc(model_w2v_doc.most_similar_vec(model_w2v_doc.vector_dict[438], 2)[1][0], reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-dream",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
