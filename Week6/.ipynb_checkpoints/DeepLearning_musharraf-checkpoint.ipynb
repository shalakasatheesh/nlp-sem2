{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbassignment": {
     "type": "header"
    },
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4fd6b7550484ed709bca7da18028042",
     "grade": false,
     "grade_id": "template_886979f3_0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Wednesday at 15:00</b>.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to two people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbassignment": {
     "type": "group_info"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the UID of each team member into the variables. \n",
    "If you work alone please leave the second variable empty.\n",
    "'''\n",
    "member1 = ''\n",
    "member2 = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d5294afb63701d0282a98e5d2d27efe",
     "grade": false,
     "grade_id": "EncoderClassifier_AEncoderClassifier_BEncoderClassifier_CEncoderClassifier_DEncoderClassifier_EEncoderClassifier_FEncoderClassifier_G_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Yelp Classifier\n",
    "\n",
    "You are given a set of $1,600$ Yelp reviews. Of these $800$ are rated with $5$ stars and the remaining $800$ with $1$ star.\n",
    "\n",
    "We now want to train a classifier using a LSTM to determine if a review is positive ($5$ stars) or negative ($1$ star). Each review is a dictionary with the fields ```text``` and ```stars```. The field ```text``` contains the tokenized review, the field ```stars``` contains the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c26b3ce2681ef45a1a057e625f0eeac",
     "grade": false,
     "grade_id": "EncoderClassifier_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import (Input, Dense, Activation, Embedding, LSTM)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setting a random seed for better reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "513f0931ad70158e4aad942ad38e0853",
     "grade": false,
     "grade_id": "EncoderClassifier_A_Description1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['10', 'pm', 'on', 'a', 'super', 'bowl', 'Sunday', 'and', 'they', \"'re\", 'already', 'closed', '?', '?', 'Weak', ',', 'no', 'wonder', 'the', 'hard', 'Rock', 'is', 'dying', 'off', '...'], 'stars': 1}\n"
     ]
    }
   ],
   "source": [
    "with open('/srv/shares/NLP/yelp_binary.pkl', 'rb') as f:\n",
    "    reviews = pickle.loads(f.read())\n",
    "    \n",
    "# Show a sample review\n",
    "print(reviews[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5ed7709bf500e661bc7a21048557e33",
     "grade": false,
     "grade_id": "EncoderClassifier_A_Description2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Yelp Classifier A) [5 points]\n",
    "\n",
    "### Preparing the data\n",
    "\n",
    "Take the reviews and create the following two lists / arrays:\n",
    "\n",
    "- ```X```: Contains the tokenized reviews (list of lists of strings)\n",
    "- ```Y```: Contains the rating of the review (use $0$ for reviews with $1$ star and 1 for reviews with $5$ stars)\n",
    "\n",
    "Next we split the data into a train and a test set using the function ```train_test_split``` from ```sklearn```. We use a test_size of $0.15$ ($15\\%$) and store these in the variables ```X_train```, ```Y_train```, ```X_test```, ```Y_test```. We also set the random state to our seed (see imports cell) for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5d861ff3554d34bfc0fbb3145191df9",
     "grade": false,
     "grade_id": "EncoderClassifier_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360\n",
      "240\n",
      "(1360,)\n",
      "(240,)\n"
     ]
    }
   ],
   "source": [
    "X = []         # This should be a list of lists of strings\n",
    "Y = [] # This should be a numpy array\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for review in reviews:\n",
    "    X.append(review['text'])\n",
    "    if review['stars'] == 1:\n",
    "        Y.append(0)\n",
    "    else:\n",
    "        Y.append(1)\n",
    "Y = np.array(Y)\n",
    "# raise NotImplementedError()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=seed)\n",
    "\n",
    "\n",
    "print(len(X_train))  # Should be 1360\n",
    "print(len(X_test))   # Should be  240\n",
    "print(Y_train.shape) # Should print (1360,)\n",
    "print(Y_test.shape)  # Should print (240,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac85fd02fff148648f456c0221a9159e",
     "grade": true,
     "grade_id": "test_EncoderClassifier_A0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a67d7250f28ebccc903f666086bb7b7",
     "grade": false,
     "grade_id": "EncoderClassifier_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Yelp Classifier B) [5 points]\n",
    "\n",
    "### Tokenize reviews into integer indexes\n",
    "\n",
    "Next we need to tokenize the reviews into integer indexes. For this we can use the build in tokenizer from keras (```tensorflow.keras.preprocessing.text.Tokenizer```). We only want to take the $5000$ most frequent words for this, ignoring all others.\n",
    "\n",
    "The tokenizer has the following methods we will use:\n",
    "\n",
    "- ```__init__(self, num_words)```: Constructor that takes the number of most frequent words we want to keep\n",
    "- ```fit_on_texts(self, data)```: Fit the tokenizer on the data (e.g. our train inputs)\n",
    "- ```texts_to_sequences(self, texts)```: Convert a list of texts or list of tokenized reviews to a list of integer indexes\n",
    "- ```sequences_to_texts(self, sequences)```: Convert a list of sequences back to texts\n",
    "\n",
    "**Attention:** We only want to fit our tokenizer on the train reviews, not on the test reviews.\n",
    "\n",
    "Your task is now to:\n",
    "\n",
    "1. Create a tokenizer that only keeps the $5000$ most frequent words\n",
    "2. Fit the tokenizer on our training data\n",
    "3. Convert the training data (```X_train```) to integer sequences and store these in the variable ```train_sequences```.\n",
    "4. Convert the test data (```X_test```) to integer sequences and store these in the variable ```test_sequences```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cee42eb4db16ee2a511dba2ec1bba369",
     "grade": false,
     "grade_id": "EncoderClassifier_B",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 132, 9, 118, 2641, 1007, 9, 61, 15, 139, 393, 5, 3, 159, 1, 61, 1267, 148, 1818, 3, 781, 9, 2642, 61, 34, 25, 38, 37, 139, 7, 152, 2641, 1007, 3905, 2, 6, 394, 42, 9]\n",
      "\n",
      "[\"great experience ! love amanda r ! she is always sweet , and friendly . she goes way above and beyond ! mornings she 's not there are always a little amanda r adds the to working out !\"]\n",
      "\n",
      "['Great', 'experience', '!', 'Love', 'Amanda', 'R', '!', 'She', 'is', 'always', 'sweet', ',', 'energetic', 'and', 'friendly', '.', 'She', 'goes', 'way', 'above', 'and', 'beyond', '!', 'Mornings', 'she', \"'s\", 'not', 'there', 'are', 'always', 'a', 'little', 'duller-', 'Amanda', 'R', 'adds', 'the', 'sparkle', 'to', 'working', 'out', '!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000 + 1\n",
    "tokenizer = Tokenizer(vocab_size)\n",
    "\n",
    "training_sequences = [[]]\n",
    "test_sequences = [[]]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# To make sure everything worked print a training example (this should print a list of integers)\n",
    "print(train_sequences[3])\n",
    "print()\n",
    "# Next take this list of integers and convert it back to a text\n",
    "print(tokenizer.sequences_to_texts([train_sequences[3]]))\n",
    "print()\n",
    "# Compare this to the corresponding training review\n",
    "print(X_train[3])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96d1df82ca74b4e8ef4ca06284efd8d4",
     "grade": true,
     "grade_id": "test_EncoderClassifier_B0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ecab17c504478be449ebbeab45111272",
     "grade": false,
     "grade_id": "EncoderClassifier_C_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Yelp Classifier C) [15 points]\n",
    "\n",
    "### Converting the sequences to sequences of equal length\n",
    "\n",
    "Since the reviews vary in length, the training sequences will have different lengths.\n",
    "\n",
    "We want to make sure all training sequences have the same length. The length of our sequences is a hyperparameter. \n",
    "\n",
    "#### 1) Determining a maximum sequence length [10 points]\n",
    "\n",
    "To determine a good value for the maximum sequence length we can plot a histogram of sequence lengths, together with some statistical parameters (mean, median, $90\\%$ quantile).\n",
    "\n",
    "Here we can pick the $90\\%$ quantile (you can use ```numpy.quantile``` for this).\n",
    "\n",
    "Please plot the histrogram of the train sequence lengths. Add the mean, median and $90\\%$ quantile as vertical lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2d915a60112a1014dec1dde36935e42",
     "grade": true,
     "grade_id": "EncoderClassifier_C",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer\n",
    "\n",
    "# Please write your code answer here!\n",
    "\n",
    "# YOUR CODE HERE\n",
    "length_train_sequence = [len(seq) for seq in train_sequences]\n",
    "mean_seq = np.mean(length_train_sequence)\n",
    "median_seq = np.median(length_train_sequence)\n",
    "# The 90% quantile should be around 270 \n",
    "quantile_90 = np.quantile(length_train_sequence, 0.90)\n",
    "\n",
    "#plotting\n",
    "plt.figure(figsize = (10,10))\n",
    "n, _, _ = plt.hist(length_train_sequence, bins = 10)\n",
    "plt.vlines(mean_seq,0,max(n)+100, color = 'red', label = 'Mean')\n",
    "plt.vlines(median_seq,0,max(n)+100, color = 'green', label = 'Median')\n",
    "plt.vlines(quantile_90,0,max(n)+100, color = 'purple', label = '90% Quantile')\n",
    "plt.ylim(0,max(n) + 100)\n",
    "plt.xlabel(\"Length of Training Sequences\")\n",
    "plt.ylabel(\"Count of Training Sequences\")\n",
    "plt.title(\"Histogram of Length of Training Sequences\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60f681272ee4374500b864df74dde7ed",
     "grade": false,
     "grade_id": "EncoderClassifier_D_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2) Padding / truncating the sequences [5 points]\n",
    "\n",
    "We found out that $90\\%$ of the reviews have a length of $\\leq 270$. \n",
    "\n",
    "Pad the training and test sequences using the function ```tensorflow.keras.preprocessing.sequence.pad_sequences```.\n",
    "\n",
    "Store these in the variables:\n",
    "\n",
    "- ```padded_train_sequences```\n",
    "- ```padded_test_sequences```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20fdb0bdb26f682490eafc02716b0275",
     "grade": false,
     "grade_id": "EncoderClassifier_D",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1360, 270)\n",
      "(240, 270)\n"
     ]
    }
   ],
   "source": [
    "padded_train_sequences = np.array([[]])\n",
    "padded_test_sequences = np.array([[]])\n",
    "# YOUR CODE HERE\n",
    "max_length = int(np.ceil(quantile_90))\n",
    "padded_train_sequences = pad_sequences(train_sequences, max_length) \n",
    "padded_test_sequences = pad_sequences(test_sequences, max_length) \n",
    "\n",
    "\n",
    "print(padded_train_sequences.shape) # Should print (1360, 270)\n",
    "print(padded_test_sequences.shape)  # Should print  (240, 270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4713fcbf685764f72a77eb70768e00dc",
     "grade": true,
     "grade_id": "test_EncoderClassifier_D0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "198be7f7878d3b9e997bbc493d2341e4",
     "grade": false,
     "grade_id": "EncoderClassifier_E_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Yelp Classifier D)\n",
    "\n",
    "### Define our model\n",
    "\n",
    "Our model should consist of the following layers:\n",
    "\n",
    "- Embedding layer with output dimension $32$\n",
    "- LSTM layer with output dimension $8$\n",
    "- Fully connected layer with a single output and sigmoid activation function\n",
    "\n",
    "Store your model in the variable ```yelp_model``` and print a summary.\n",
    "\n",
    "Your summary should look similar to this:\n",
    "\n",
    "```\n",
    "Model: \"Yelp_Model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(None, 270)]             0         \n",
    "_________________________________________________________________\n",
    "embedding (Embedding)        (None, 270, 16)           80016     \n",
    "_________________________________________________________________\n",
    "lstm (LSTM)                  (None, 8)                 800       \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 1)                 9         \n",
    "=================================================================\n",
    "Total params: 80,825\n",
    "Trainable params: 80,825\n",
    "Non-trainable params: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2579741cf9de8a0672fbd3848e205d99",
     "grade": true,
     "grade_id": "EncoderClassifier_E",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Yelp_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 270)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 270, 32)           160032    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 1312      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 161,353\n",
      "Trainable params: 161,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "yelp_model = None\n",
    "\n",
    "input_obj = Input(shape = (max_length, ), name = 'Input')\n",
    "\n",
    "embed_layer = Embedding(vocab_size, 32, name = 'embedding')\n",
    "y_embed = embed_layer(input_obj)\n",
    "\n",
    "lstm_layer = LSTM(8, name = 'lstm')\n",
    "y_lstm = lstm_layer(y_embed)\n",
    "\n",
    "output_layer = Dense(1, activation = 'sigmoid', name = 'Output')\n",
    "y = output_layer(y_lstm)\n",
    "\n",
    "yelp_model = Model(input_obj, y, name = 'Yelp_model')\n",
    "yelp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7801dfab4669b6592cb88769c098e4a4",
     "grade": false,
     "grade_id": "EncoderClassifier_F_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Yelp Classifier E)\n",
    "\n",
    "### Compile our model\n",
    "\n",
    "Finally we need to compile our model.\n",
    "\n",
    "Use the binary crossentropy loss, the optimizer Adam and accuracy as a metric.\n",
    "\n",
    "Next fit the model to our training data using a batch size of $32$ for a total of $10$ epochs.\n",
    "\n",
    "**Attention:** Each time you execute this cell the model will be trained more (e.g. execute it once and we train for $10$ epochs, execute it twice and we train for $20$ epochs, ...). To reset the model execute the cell above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "557010c4135cd6b1b52ae2e39f4766cd",
     "grade": true,
     "grade_id": "EncoderClassifier_F",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43/43 [==============================] - 6s 97ms/step - loss: 0.6924 - accuracy: 0.5429\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 0.6717 - accuracy: 0.7403\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 0.5519 - accuracy: 0.8630\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 4s 95ms/step - loss: 0.3842 - accuracy: 0.9675\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 4s 98ms/step - loss: 0.2840 - accuracy: 0.9735\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 4s 95ms/step - loss: 0.2075 - accuracy: 0.9786\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 0.1397 - accuracy: 0.9950\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 4s 98ms/step - loss: 0.0986 - accuracy: 0.9992\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 4s 97ms/step - loss: 0.1030 - accuracy: 0.9868\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 4s 96ms/step - loss: 0.0618 - accuracy: 0.9970\n",
      "\n",
      "Test results - Loss: 0.3892 - Accuracy: 87.92%\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "\n",
    "# Please write your code answer here!\n",
    "\n",
    "# YOUR CODE HERE\n",
    "yelp_model.compile(\n",
    "    loss = BinaryCrossentropy(),\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history = yelp_model.fit(\n",
    "    x = padded_train_sequences,\n",
    "    y = Y_train,\n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    verbose = 1\n",
    ")\n",
    "# Test the model after training\n",
    "test_results = yelp_model.evaluate(padded_test_sequences, Y_test, verbose=False)\n",
    "\n",
    "print()\n",
    "print(f'Test results - Loss: {test_results[0]:.4f} - Accuracy: {100*test_results[1]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f1064dd3e5d29d6cc8d34a786e250cf",
     "grade": false,
     "grade_id": "EncoderClassifier_G_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Yelp Classifier E) [15 points]\n",
    "\n",
    "### Bonus Task - Extract the embeddings\n",
    "\n",
    "This is a bonus task. You do not need to solve it to get $100\\%$ on this assignment.\n",
    "\n",
    "Extract the embeddings from the embedding layer of your model and create a dictionary for the embeddings.\n",
    "\n",
    "The embeddings are stored in the weight matrix of the embedding layer (```get_weights()```).\n",
    "\n",
    "To find out which row of the embedding matrix corresponds to which word we need to use the word_index of our tokenizer. This is a dictionary containts the words (tokens) as keys and their index as a value. We only look at the first $5000$ entries of the word index.\n",
    "\n",
    "Next create a VectorModel (see Assignment 4) with these embeddings and print the most similar words to the following input words:\n",
    "\n",
    "```['good', 'bad', 'fantastic', 'poor', 'amazing']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64b67cfef01dbf56da53ab65a4dd87d5",
     "grade": true,
     "grade_id": "EncoderClassifier_G",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "For word:  good\n",
      "[('nicest', 0.89318201), ('friendly', 0.88685091), ('loved', 0.8839231)]\n",
      " \n",
      "For word:  bad\n",
      "[('wonder', 0.92034404), ('person', 0.91320252), ('rude', 0.90867129)]\n",
      " \n",
      "For word:  fantastic\n",
      "[('definitely', 0.95223944), ('friendly', 0.95115422), ('excellent', 0.93171583)]\n",
      " \n",
      "For word:  poor\n",
      "[('wonder', 0.94352012), ('read', 0.9408919), ('sick', 0.93549593)]\n",
      " \n",
      "For word:  amazing\n",
      "[('best', 0.96124598), ('lots', 0.95868081), ('helpful', 0.94770745)]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "\n",
    "# Please write your code answer here!\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "# yelp_model.layers[1].get_weights() == embed_layer.get_weights()[0]\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "   \n",
    "class VectorModel:\n",
    "    \n",
    "    def __init__(self, vector_dict: Dict[str, np.ndarray]):\n",
    "        \n",
    "        self.vector_embedd = vector_dict\n",
    "        #finding shape of vector embeddings to save all embeddings in a array\n",
    "        shape_dict = self.vector_size()\n",
    "        #embedding array and index dict\n",
    "        embed_array = np.zeros((len(self.vector_embedd.keys()), shape_dict))\n",
    "        self.inverse_index_dict = dict()\n",
    "        for index, word in enumerate(self.vector_embedd.keys()):\n",
    "            self.inverse_index_dict[index] = word\n",
    "            embed_array[index,:] = self.vector_embedd[word]\n",
    "        #finding the normalized array\n",
    "        self.normalize_embed_array = embed_array/np.linalg.norm(embed_array, axis = 1).reshape(-1,1)\n",
    "        #self.cos_sim = np.dot(self.normalize_embed_array, self.normalize_embed_array.T)\n",
    "\n",
    "        \n",
    "        \n",
    "    def embed(self, word: str) -> np.ndarray:\n",
    "        if word in self.vector_embedd.keys():\n",
    "            return self.vector_embedd[word]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def vector_size(self) -> int:\n",
    "        #num_of_words = len(self.vector_embedd.keys())\n",
    "        first_word = list(self.vector_embedd.keys())[0]\n",
    "        lenth_of_embed = len(self.vector_embedd[first_word])\n",
    "        return lenth_of_embed\n",
    "    \n",
    "    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        vec1_mag = np.linalg.norm(vec1)\n",
    "        vec2_mag = np.linalg.norm(vec2)\n",
    "        return np.dot(vec1, vec2.T)/(vec1_mag * vec2_mag)\n",
    "       \n",
    "\n",
    "    def most_similar(self, word: str, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "        word_embedd = self.vector_embedd[word]\n",
    "        #finding unit vector\n",
    "        word_embedd_norm = word_embedd/np.linalg.norm(word_embedd)\n",
    "        #finding similarity from all vectors\n",
    "        cosine_simi = word_embedd_norm @ self.normalize_embed_array.T\n",
    "        #getting the highest similarity score index\n",
    "        sorted_simi_desc = np.argsort(cosine_simi)[::-1]\n",
    "        list_simi_words = []\n",
    "        #finding most similar words\n",
    "        for sim_index in sorted_simi_desc[1:top_n + 1]:\n",
    "            word = self.inverse_index_dict[sim_index]\n",
    "            list_simi_words.append((word, np.round(cosine_simi[sim_index],8)))\n",
    "            \n",
    "        return list_simi_words\n",
    "        \n",
    "    def most_similar_vec(self, vec: np.ndarray, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "        #finding unit vector\n",
    "        word_embedd_norm = vec/np.linalg.norm(vec)\n",
    "        #finding similarity from all vectors\n",
    "        cosine_simi = word_embedd_norm @ self.normalize_embed_array.T\n",
    "        #getting the highest similarity score index\n",
    "        sorted_simi_desc = np.argsort(cosine_simi)[::-1]\n",
    "        list_simi_words = []\n",
    "        #finding most similar words\n",
    "        for sim_index in sorted_simi_desc[:top_n]:\n",
    "            word = self.inverse_index_dict[sim_index]\n",
    "            list_simi_words.append((word, np.round(cosine_simi[sim_index],8)))\n",
    "            \n",
    "        return list_simi_words\n",
    "    \n",
    "weights_matr = yelp_model.layers[1].get_weights()[0]\n",
    "embedding_dic = {}\n",
    "for key, value in tokenizer.word_index.items():\n",
    "    if value <=5000:\n",
    "        embedding_dic[key] = weights_matr[value,:]\n",
    "        \n",
    "model_w2v = VectorModel(embedding_dic)\n",
    "input_words = ['good', 'bad', 'fantastic', 'poor', 'amazing']\n",
    "print(\"------------------------------\")\n",
    "for word in input_words:\n",
    "    print(\"For word: \", word)\n",
    "    print(model_w2v.most_similar(word, 3))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21aa3025388cfd3d016fed137737c926",
     "grade": false,
     "grade_id": "EncoderDecoder_AEncoderDecoder_BEncoderDecoder_CEncoderDecoder_D_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Sequence to Sequence\n",
    "\n",
    "We want to train a sequence to sequence model, consisting of an LSTM encoder and an LSTM decoder.\n",
    "\n",
    "For this we use a dummy task that consists of translating date strings from one format to another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09a8384f5cea69e09a301384d31d2640",
     "grade": false,
     "grade_id": "EncoderDecoder_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Embedding, Dense, LSTM, \n",
    "                                     TimeDistributed, Input, RepeatVector, Activation)\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecce8e860c6b38dedd41a59047405736",
     "grade": false,
     "grade_id": "EncoderDecoder_A_Description1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2', '1', '-', '0', '6', '-', '1', '8', '8', '6'], ['1', '2', '-', '0', '8', '-', '1', '3', '8', '1'], ['1', '8', '-', '1', '2', '-', '1', '0', '2', '2']]\n",
      "[['1', '8', '8', '6', '-', '0', '6', '-', '2', '1'], ['1', '3', '8', '1', '-', '0', '8', '-', '1', '2'], ['1', '0', '2', '2', '-', '1', '2', '-', '1', '8']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def translate_date(day, month, year):\n",
    "    return (\n",
    "        f'{day:02}-{month:02}-{year}',\n",
    "        f'{year:04}-{month:02}-{day:02}'\n",
    "    )\n",
    "\n",
    "def create_sample():\n",
    "    year = rd.choice(years)\n",
    "    day = rd.choice(days)\n",
    "    month = rd.choice(months)\n",
    "    return translate_date(day, month, year)\n",
    "\n",
    "def create_samples(n_samples=5000):\n",
    "    samples = set()\n",
    "    while len(samples) < n_samples:\n",
    "        samples.add(create_sample())\n",
    "    return list(samples)\n",
    "\n",
    "years = range(1000, 2022)\n",
    "days = range(1, 29)\n",
    "months = range(1, 13)\n",
    "\n",
    "N_samples = 10000\n",
    "\n",
    "samples = create_samples(N_samples)\n",
    "\n",
    "# Split into X and Y\n",
    "X, Y = zip(*samples)\n",
    "X = [list(x) for x in X]\n",
    "Y = [list(y) for y in Y]\n",
    "\n",
    "# Train / Test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Print the first three inputs and first three outputs\n",
    "print(X_train[:3])\n",
    "print(Y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "241623a203f203a751791c966dd811ab",
     "grade": false,
     "grade_id": "EncoderDecoder_A_Description2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sequence to Sequence A) [5 points]\n",
    "\n",
    "### Tokenize the data using keras built-in tokenizer.\n",
    "\n",
    "Since our outputs are also sequences, we need to fit our tokenizer on the combined input and output (```X_train + Y_train```).\n",
    "\n",
    "Finally create the four variables ```X_train_sequences```, ```Y_train_sequences```, ```X_test_sequences``` and ```Y_test_sequences``` from your train and test data.\n",
    "\n",
    "Since all inputs and outputs have the same lenght, we do not need to apply any padding / truncating of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "593901258012e65dc17e889ba59cd6ca",
     "grade": false,
     "grade_id": "EncoderDecoder_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 10)\n",
      "(8500, 10)\n",
      "(1500, 10)\n",
      "(1500, 10)\n",
      "{'1': 1, '-': 2, '0': 3, '2': 4, '4': 5, '6': 6, '3': 7, '5': 8, '8': 9, '7': 10, '9': 11}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "combined_list = []\n",
    "combined_list.extend(X_train)\n",
    "combined_list.extend(Y_train)\n",
    "tokenizer.fit_on_texts(combined_list)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "Y_train_sequences = tokenizer.texts_to_sequences(Y_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "Y_test_sequences = tokenizer.texts_to_sequences(Y_test)\n",
    "# Make sure we are working with numpy arrays:\n",
    "\n",
    "X_train_sequences = np.array(X_train_sequences)\n",
    "Y_train_sequences = np.array(Y_train_sequences)\n",
    "X_test_sequences = np.array(X_test_sequences)\n",
    "Y_test_sequences = np.array(Y_test_sequences)\n",
    "\n",
    "print(X_train_sequences.shape) # Should print (4250, 10)\n",
    "print(Y_train_sequences.shape) # Should print (4250, 10)\n",
    "\n",
    "print(X_test_sequences.shape)  # Should print (750, 10)\n",
    "print(Y_test_sequences.shape)  # Should print (750, 10)\n",
    "\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e018de76fd68dab61e44bc302f09cee",
     "grade": true,
     "grade_id": "test_EncoderDecoder_A0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67cf47fba8df96008ac8d1f9158e940d",
     "grade": false,
     "grade_id": "EncoderDecoder_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sequence to Sequence B) [25 points]\n",
    "\n",
    "### Define your model\n",
    "\n",
    "Create a model consisting of:\n",
    "\n",
    "- An embedding layer with output dimensionality of $64$\n",
    "- A LSTM layer (Encoder) with outputs of size $32$\n",
    "- A LSTM layer (Decoder) with outputs of size $32$\n",
    "- A dense layer\n",
    "- An activation layer using the softmax\n",
    "\n",
    "**Attention:** You will need the ```RepeatVector``` and ```TimeDistributed``` objects.\n",
    "\n",
    "- ```RepeatVector``` takes a vector as an input and repeats it $n$ times. This is useful to repeat the output of the encoder such that the decoder is fed the last output of encoder at each time step.\n",
    "- ```TimeDistributed``` is a wrapper around a layer (e.g. a dense layer) that takes a sequence of inputs and creates a sequence of outputs by applying the layer to each element of the input sequence.\n",
    "\n",
    "Your model summary should look similar to this:\n",
    "\n",
    "```\n",
    "Model: \"Seq2Seq\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(None, 10)]              0         \n",
    "_________________________________________________________________\n",
    "embedding (Embedding)        (None, 10, 64)            768       \n",
    "_________________________________________________________________\n",
    "lstm (LSTM)                  (None, 32)                12416     \n",
    "_________________________________________________________________\n",
    "repeat_vector (RepeatVector) (None, 10, 32)            0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 10, 32)            8320      \n",
    "_________________________________________________________________\n",
    "time_distributed (TimeDistri (None, 10, 12)            396       \n",
    "_________________________________________________________________\n",
    "activation (Activation)      (None, 10, 12)            0         \n",
    "=================================================================\n",
    "Total params: 21,900\n",
    "Trainable params: 21,900\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "271e7199046b7beba2453135d34ab83e",
     "grade": true,
     "grade_id": "EncoderDecoder_B",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Seq2Seq\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 10, 64)            768       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 32)            8320      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 12)            396       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10, 12)            0         \n",
      "=================================================================\n",
      "Total params: 21,900\n",
      "Trainable params: 21,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "\n",
    "# Please write your code answer here!\n",
    "\n",
    "# YOUR CODE HERE\n",
    "seq2seq_model = None\n",
    "\n",
    "input_obj = Input(shape = (10, ), name = 'Input')\n",
    "\n",
    "embed_layer = Embedding(len(tokenizer.word_index)+1, 64)\n",
    "y_embed = embed_layer(input_obj)\n",
    "\n",
    "lstm_layer = LSTM(32)\n",
    "y_lstm = lstm_layer(y_embed)\n",
    "\n",
    "rep_vector = RepeatVector(10)\n",
    "rep_vec_layer = rep_vector(y_lstm)\n",
    "\n",
    "lstm_layer = LSTM(32, return_sequences=True)\n",
    "y_lstm = lstm_layer(rep_vec_layer)\n",
    "\n",
    "dense_layer = TimeDistributed(Dense(12))\n",
    "dense_op = dense_layer(y_lstm)\n",
    "\n",
    "activation_layer = Activation('softmax')\n",
    "y = activation_layer(dense_op)\n",
    "\n",
    "seq2seq_model = Model(input_obj, y, name = 'Seq2Seq')\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "417b7b1eb6535b7fdea735b79eee9704",
     "grade": false,
     "grade_id": "EncoderDecoder_C_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sequence to Sequence C) [5 points]\n",
    "\n",
    "### Compile your model\n",
    "\n",
    "Compile your model using adam as an optimizer, accuracy as a metric and sparse_categorical_crossentropy as your loss.\n",
    "\n",
    "Then fit your model on the training data using $20$ epochs and a batch size of $32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94b7054135731536c20195730e20f88b",
     "grade": true,
     "grade_id": "EncoderDecoder_C",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "266/266 [==============================] - 7s 12ms/step - loss: 2.1918 - accuracy: 0.2680\n",
      "Epoch 2/20\n",
      "266/266 [==============================] - 3s 12ms/step - loss: 1.5729 - accuracy: 0.4628\n",
      "Epoch 3/20\n",
      "266/266 [==============================] - 3s 12ms/step - loss: 1.0677 - accuracy: 0.6286\n",
      "Epoch 4/20\n",
      "266/266 [==============================] - 3s 13ms/step - loss: 0.9147 - accuracy: 0.6760\n",
      "Epoch 5/20\n",
      "266/266 [==============================] - 3s 12ms/step - loss: 0.7923 - accuracy: 0.7157\n",
      "Epoch 6/20\n",
      "266/266 [==============================] - 3s 12ms/step - loss: 0.6884 - accuracy: 0.7466\n",
      "Epoch 7/20\n",
      "266/266 [==============================] - 3s 12ms/step - loss: 0.6038 - accuracy: 0.7739\n",
      "Epoch 8/20\n",
      "244/266 [==========================>...] - ETA: 0s - loss: 0.5265 - accuracy: 0.8024"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "seq2seq_model.compile(\n",
    "    loss = SparseCategoricalCrossentropy(),\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history = seq2seq_model.fit(\n",
    "    x = X_train_sequences,\n",
    "    y = Y_train_sequences,\n",
    "    epochs = 20,\n",
    "    batch_size = 32,\n",
    "    verbose = 1\n",
    ")\n",
    "# Test the model after training\n",
    "test_results = seq2seq_model.evaluate(X_test_sequences, Y_test_sequences, verbose=False)\n",
    "print()\n",
    "print(f'Test results - Loss: {test_results[0]:.4f} - Accuracy: {100*test_results[1]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e768cc721ffd75054f73c6dbbad95b0",
     "grade": false,
     "grade_id": "EncoderDecoder_D_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sequence to Sequence D) [10 points]\n",
    " \n",
    "### Use your model for inference\n",
    "\n",
    "We now want to use the model to convert a single date (```20-05-1973```).\n",
    "\n",
    "This process consists of the following steps:\n",
    "\n",
    "1. Convert the date to a sequence.\n",
    "2. Predict the output of the model for that sequence\n",
    "3. Print the predicted date\n",
    "\n",
    "The prediction will be a sequence of vectors, where each element in the vector corresponds to the probability of that vector representing the token with that index. \n",
    "\n",
    "Example:\n",
    "\n",
    "The output of your model would be the ten vectors $[v_0, v_1, v_2, ..., v_9]$. Now each vector has 12 elements:\n",
    "\n",
    "$v_i = [a_0, a_1, ... ,a_{10}, a_{11}]$, $\\quad\\sum_{i=0}^{11} a_i = 1$. (Because of applying the softmax)\n",
    "\n",
    "The element $a_1$ is the probability that the predicted token is the one with index $1$. \n",
    "\n",
    "To convert each of the vectors $v_i$ to an integer index we need to find the index of the largest $a_i$ of that vector (argmax).\n",
    "\n",
    "Then we get an output sequence like $[1, 4, 2, 5, 3, 7, 6, 3, 2, 4]$. This sequence can now be converted back to a text using the method ```sequences_to_texts``` of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c06124dce7f9b940ad1a541d7f903b1",
     "grade": true,
     "grade_id": "EncoderDecoder_D",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# Please write your code answer here!\n",
    "\n",
    "# YOUR CODE HERE\n",
    "check_date = '20-05-1973'\n",
    "check_date = [x for x in check_date]\n",
    "#converting to sequence\n",
    "check_date_seq = tokenizer.texts_to_sequences([check_date])\n",
    "check_date_seq = np.array(check_date_seq)\n",
    "out = seq2seq_model.predict(check_date_seq)\n",
    "\n",
    "list_high_prob = []\n",
    "\n",
    "for elem_ind in range(out[0].shape[0]):\n",
    "    arg_max = np.argmax(out[0][elem_ind,:])\n",
    "    list_high_prob.append(arg_max)\n",
    "    \n",
    "tokenizer.sequences_to_texts([list_high_prob])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
