{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powered-slide",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbassignment": {
     "type": "header"
    },
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4fd6b7550484ed709bca7da18028042",
     "grade": false,
     "grade_id": "template_886979f3_0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Wednesday at 15:00</b>.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to two people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controversial-biography",
   "metadata": {
    "nbassignment": {
     "type": "group_info"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the UID of each team member into the variables. \n",
    "If you work alone please leave the second variable empty.\n",
    "'''\n",
    "member1 = 'Syed Mushrraf Ali (sali2s, 9040658)'\n",
    "member2 = 'Shalaka Satheesh (ssathe2s, 9040760)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-milan",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bcfc9fec57b3191b04b5977dfff4f75",
     "grade": false,
     "grade_id": "SpacyIntro_ASpacyIntro_BSpacyIntro_CSpacyIntro_DSpacyIntro_E_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Introduction to spaCy\n",
    "\n",
    "SpaCy is a tool that does tokenization, parsing, tagging and named entity regocnition (among other things).\n",
    "\n",
    "When we parse a document via spaCy, we get an object that holds sentences and tokens, as well as their POS tags, dependency relations and so on.\n",
    "\n",
    "Look at the next cell for an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "broke-parks",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32e3514f2f346e23815caffc075f05ed",
     "grade": false,
     "grade_id": "SpacyIntro_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy PROPN nsubj False\n",
      "is AUX ROOT True\n",
      "capable ADJ acomp False\n",
      "of ADP prep True\n",
      "    SPACE  False\n",
      "tagging NOUN pobj False\n",
      ", PUNCT punct False\n",
      "parsing VERB conj False\n",
      "and CCONJ cc True\n",
      "annotating VERB conj False\n",
      "text NOUN dobj False\n",
      ". PUNCT punct False\n",
      "It PRON nsubj True\n",
      "recognizes VERB ROOT False\n",
      "sentences NOUN dobj False\n",
      "and CCONJ cc True\n",
      "stop VERB conj False\n",
      "words NOUN dobj False\n",
      ". PUNCT punct False\n",
      "------------------------------\n",
      "The nouns and proper nouns in this text are:\n",
      "SpaCy\n",
      "tagging\n",
      "text\n",
      "sentences\n",
      "words\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "# Our sample input\n",
    "text = 'SpaCy is capable of    tagging, parsing and annotating text. It recognizes sentences and stop words.'\n",
    "\n",
    "# Parse the sample input\n",
    "doc = nlp(text)\n",
    "\n",
    "# For every sentence\n",
    "for sent in doc.sents:\n",
    "    # For every token\n",
    "    for token in sent:\n",
    "        # Print the token itself, the pos tag, \n",
    "        # dependency tag and whether spacy thinks this is a stop word\n",
    "        print(token, token.pos_, token.dep_, token.is_stop)\n",
    "        \n",
    "print('-'*30)\n",
    "print('The nouns and proper nouns in this text are:')\n",
    "# Print only the nouns:\n",
    "for token in doc:\n",
    "    if token.pos_ in ['NOUN', 'PROPN']:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-waters",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2eaf9bed1ec41ba3dafd392c8aae894",
     "grade": false,
     "grade_id": "SpacyIntro_A_Description1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## SpaCy A) [5 points]\n",
    "### Splitting text into sentences\n",
    "\n",
    "You are given the text in the next cell.\n",
    "\n",
    "```\n",
    "text = '''\n",
    "This is a sentence. \n",
    "Mr. A. said this was another! \n",
    "But is this a sentence? \n",
    "The abbreviation Merch. means merchant(s).\n",
    "At certain univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "```\n",
    "\n",
    "Use spaCy to split this into sentences. Store the resulting sentences (each as a **single** string) in the list ```sentences```. Make sure to convert the tokens to strings (e.g. via str(token))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broken-fifth",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90cbf65cd82368f51da27e28808346b2",
     "grade": false,
     "grade_id": "SpacyIntro_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is a sentence.\n",
      ".\n",
      "Mr. A. said this was another! \n",
      "\n",
      ".\n",
      "But is this a sentence?\n",
      ".\n",
      "The abbreviation Merch. means merchant(s).\n",
      "\n",
      ".\n",
      "At certain Univ.\n",
      ".\n",
      "in the U.S. and U.K. they study NLP.\n",
      "\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "sentences = []\n",
    "tokens = []\n",
    "\n",
    "doc = nlp(text)\n",
    "for sentence in (doc.sents):\n",
    "    sentences.append(str(sentence))\n",
    "    for token in sentence:\n",
    "        tokens.append(str(token))\n",
    "    \n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print('.')\n",
    "    assert type(sentence) == str, 'You need to convert this to a single string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hydraulic-packet",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbbce90ee40e45a30a046006e4087da1",
     "grade": true,
     "grade_id": "test_SpacyIntro_A0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-mississippi",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a49ebe93c151344970ab83081b2a413f",
     "grade": false,
     "grade_id": "SpacyIntro_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## SpaCy B) [5 points]\n",
    "\n",
    "### Cluster the text by POS tag\n",
    "\n",
    "Next we want to cluster the text by the corresponding part-of-speech (POS) tags. \n",
    "\n",
    "The result should be a dictionary ```pos_tags``` where the keys are the POS tags and the values are lists of words with those POS tags. Make sure your words are converted to **strings**.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "```\n",
    "pos_tags['VERB'] # Output: ['said', 'means', 'study']\n",
    "pos_tags['ADJ']  # Output: ['certain']\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "swedish-protein",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92630877dfeae43e9869b293e17da2b0",
     "grade": false,
     "grade_id": "SpacyIntro_B",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words with the POS tag SPACE are ['\\n', '\\n', '\\n', '\\n'].\n",
      "The words with the POS tag DET are ['This', 'a', 'this', 'another', 'this', 'a', 'The', 'the'].\n",
      "The words with the POS tag AUX are ['is', 'was', 'is'].\n",
      "The words with the POS tag NOUN are ['sentence', 'sentence', 'abbreviation'].\n",
      "The words with the POS tag PUNCT are ['.', '!', '?', '.', ')', '.', '.', '.'].\n",
      "The words with the POS tag PROPN are ['Mr.', 'A.', 'Merch', 'merchant(s', 'Univ', 'U.S.', 'U.K.', 'NLP'].\n",
      "The words with the POS tag VERB are ['said', 'means', 'study'].\n",
      "The words with the POS tag CCONJ are ['But', 'and'].\n",
      "The words with the POS tag ADP are ['At', 'in'].\n",
      "The words with the POS tag ADJ are ['certain'].\n",
      "The words with the POS tag PRON are ['they'].\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "pos_tags = dict()\n",
    "\n",
    "doc = nlp(text)\n",
    "        \n",
    "for sentence in doc.sents:\n",
    "    for token in sentence:\n",
    "        if token.pos_ not in pos_tags:\n",
    "            pos_tags[token.pos_] = []\n",
    "        pos_tags[token.pos_].append(str(token))\n",
    "\n",
    "for key in pos_tags:\n",
    "    print('The words with the POS tag {} are {}.'.format(key, pos_tags[key]))\n",
    "    for token in pos_tags[key]:\n",
    "        assert type(token) == str, 'Each token should be a string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "joined-confirmation",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "401366109e209a58b9d6880f9d0efbd2",
     "grade": true,
     "grade_id": "test_SpacyIntro_B0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-family",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "764be63962b4a589a22a7118b6196948",
     "grade": false,
     "grade_id": "SpacyIntro_C_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SpaCy C) [5 points]\n",
    "\n",
    "### Stop word removal\n",
    "\n",
    "Stop words are words that appear often in a language and don't hold much meaning for a NLP task. Examples are the words ```a, to, the, this, has, ...```. This depends on the task and domain you are working on.\n",
    "\n",
    "SpaCy has its own internal list of stop words. Use spaCy to remove all stop words from the given text. Store your result as a **single string** in the variable ```stopwords_removed```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "consecutive-circular",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8ade4666109405cdaf97edbe7b9fb3f",
     "grade": false,
     "grade_id": "SpacyIntro_C",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a this was another But is this a The At in the and they \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "stopwords_removed = ''\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    for token in sentence:\n",
    "        if token.is_stop:\n",
    "            stopwords_removed += str(token)\n",
    "            stopwords_removed += ' '\n",
    "\n",
    "print(stopwords_removed)\n",
    "assert type(stopwords_removed) == str, 'Your answer should be a single string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "industrial-victorian",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ede061b3d5f7730b3d8a61b49d24c668",
     "grade": true,
     "grade_id": "test_SpacyIntro_C0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-instruction",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09f8b638ddc3e45a920a8632957fb8da",
     "grade": false,
     "grade_id": "SpacyIntro_D_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SpaCy D) [2 points]\n",
    "\n",
    "### Dependency Tree\n",
    "\n",
    "We now want to use spaCy to visualize the dependency tree of a certain sentence. Look at the Jupyter Example on the [spaCy website](https://spacy.io/usage/visualizers/). Render the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spiritual-cleaner",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "789a34dc995bc1fbcc1077b5a07edcbd",
     "grade": true,
     "grade_id": "SpacyIntro_D",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1d4ec7beb0ef41c3b1bba2b1c58417e9-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Dependency</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Parsing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">helpful</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">many</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">tasks.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1d4ec7beb0ef41c3b1bba2b1c58417e9-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "text = 'Dependency Parsing is helpful for many tasks.'\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-democracy",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a5b58876deddef3e637584ebfbace3c",
     "grade": false,
     "grade_id": "SpacyIntro_E_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SpaCy E) [5 points]\n",
    "\n",
    "### Dependency Parsing\n",
    "\n",
    "Use spaCy to extract all subjects and objects from the text. We define a subject as any word that has ```subj``` in its dependency tag (e.g. ```nsubj```, ```nsubjpass```, ...). Similarly we define an object as any token that has ```obj``` in its dependency tag (e.g. ```dobj```, ```pobj```, etc.).\n",
    "\n",
    "For each sentence extract the subject, root node ```ROOT``` of the tree and object and store them as a single string in a list. Name this list ```subj_obj```.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "```\n",
    "text = 'Learning multiple ways of representing text is cool. We can access parts of the sentence with dependency tags.'\n",
    "\n",
    "subj_obj = ['Learning ways text is', 'We access parts sentence tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "retired-assist",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7aa38180840293c1d230a57eb90d9ee7",
     "grade": false,
     "grade_id": "SpacyIntro_E",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is \n",
      "A. said this \n",
      "is this \n",
      "abbreviation means \n",
      "At Univ \n",
      "U.S. they study NLP \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "subj_obj = []\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# USE re.compile ????\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    sentence_string = ''\n",
    "    for token in sentence:\n",
    "        if re.findall('\\w*subj\\w*', str(token.dep_)):\n",
    "            sentence_string += str(token)\n",
    "            sentence_string += ' '\n",
    "        elif re.findall('ROOT', str(token.dep_)):\n",
    "            sentence_string += str(token)\n",
    "            sentence_string += ' '\n",
    "        elif re.findall('\\w*obj\\w*', str(token.dep_)):\n",
    "            sentence_string += str(token)\n",
    "            sentence_string += ' '\n",
    "    subj_obj.append(sentence_string)\n",
    "\n",
    "for cleaned_sent in subj_obj:\n",
    "    print(cleaned_sent)\n",
    "    assert type(cleaned_sent) == str, 'Each cleaned sentence should be a string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mediterranean-peeing",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd16138bad9d0aced1ce7819d592c375",
     "grade": true,
     "grade_id": "test_SpacyIntro_E0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-quarter",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c4282d5e277c9573f5323515a68457d",
     "grade": false,
     "grade_id": "POS_Keywords_APOS_Keywords_B_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Keyword Extraction\n",
    "\n",
    "In this assignment we want to write a keyword extractor. There are several methods of which we want to explore a few.\n",
    "\n",
    "We want to extract keywords from our Yelp reviews.\n",
    "\n",
    "##  POS tag based extraction\n",
    "\n",
    "When we look at keywords we realize that they are often combinations of nouns and adjectives. The idea is to find all sequences of nouns and adjectives in a corpus and count them. The $n$ most frequent ones are then our keywords.\n",
    "\n",
    "A keyword (or keyphrase) by this definition is any combination of nouns (NOUN) and adjectives (ADJ) that ends in a noun. We also count proper nouns (PROPN) as nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-darkness",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33f153a362b27cca8a0c0b6d8e54bde4",
     "grade": false,
     "grade_id": "POS_Keywords_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## POS tag based extraction A) [35 points]\n",
    "\n",
    "### POSKeywordExtractor\n",
    "\n",
    "Please complete the function ```keywords``` in the class ```POSKeywordExtractor```.\n",
    "\n",
    "You are given the file ```wiki_nlp.txt```, which has the raw text from all top-level Wikipedia pages under the category ```Natural language processing```. Use this for extracting your keywords.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "Let us look at the definition of an index term or keyword from Wikipedia. Here I highlighted all combinations of nouns and adjectives that end in a noun. All the highlighted words are potential keywords.\n",
    "\n",
    "An **index term**, **subject term**, **subject heading**, or **descriptor**, in **information retrieval**, is a **term** that captures the **essence** of the **topic** of a **document**. **Index terms** make up a **controlled vocabulary** for **use** in **bibliographic records**.\n",
    "\n",
    "*Rules:*\n",
    "\n",
    "- A keyphrase is a sequence of nouns, adjectives and proper nouns ending in a noun or proper noun.\n",
    "- Keywords / Keyphrases can not go over sentence boundaries.\n",
    "- We always take the longest sequence of nouns, adjectives and proper nouns\n",
    "  - Consider the sentence ```She studies natural language processing.```. The only extracted keyphrase here will be ```('natural', 'language', 'processing')```.\n",
    "- Consider the sentence ```neural networks massively increased the performance.```:\n",
    "  - Here our keyphrase would be ```neural networks```, not ```neural networks massively```.\n",
    "  - Our keyphrases are always the longest sequence of nouns and adjectives ending in a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "loaded-difficulty",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4bd75393770930b4ff0803bdc433d48",
     "grade": false,
     "grade_id": "POS_Keywords_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keyword ('words',) appears 353 times.\n",
      "The keyword ('text',) appears 342 times.\n",
      "The keyword ('example',) appears 263 times.\n",
      "The keyword ('word',) appears 231 times.\n",
      "The keyword ('natural', 'language', 'processing') appears 184 times.\n",
      "The keyword ('documents',) appears 162 times.\n",
      "The keyword ('language',) appears 148 times.\n",
      "The keyword ('information',) appears 137 times.\n",
      "The keyword ('n',) appears 136 times.\n",
      "The keyword ('set',) appears 133 times.\n",
      "The keyword ('system',) appears 120 times.\n",
      "The keyword ('t',) appears 117 times.\n",
      "The keyword ('number',) appears 112 times.\n",
      "The keyword ('sentence',) appears 112 times.\n",
      "The keyword ('context',) appears 110 times.\n",
      "CPU times: user 7.33 s, sys: 3.1 s, total: 10.4 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from typing import List, Tuple, Iterable\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class POSKeywordExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up SpaCy in a more efficient way by disabling what we do not need\n",
    "        # This is the dependency parser (parser) and the named entity recognizer (ner)\n",
    "        self.nlp = spacy.load(\n",
    "            '/srv/shares/NLP/en_core_web_sm', \n",
    "            disable=['ner', 'parser']\n",
    "        )\n",
    "        # Add the sentencizer to quickly split our text into sentences\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        # Increase the maximum length of text SpaCy can parse in one go\n",
    "        self.nlp.max_length = 1500000\n",
    "        \n",
    "    def validate_keyphrase(self, candidate: Iterable[Token]) -> Iterable[Token]:\n",
    "        '''\n",
    "        Takes in a list of tokens which are all proper nouns, nouns or adjectives\n",
    "        and returns the longest sequence that ends in a proper noun or noun\n",
    "        \n",
    "        Args:\n",
    "            candidate         -- List of spacy tokens\n",
    "        Returns:\n",
    "            longest_keyphrase -- The longest sequence that ends in a noun\n",
    "                                 or proper noun\n",
    "                                 \n",
    "        Example:\n",
    "            candidate = [neural, networks, massively]\n",
    "            longest_keyphrase = [neural, networks]\n",
    "        '''\n",
    "        # If the candidate list is not empty\n",
    "        if candidate:\n",
    "            # Check if the last word is an adjective\n",
    "            if candidate[-1].pos_ == 'ADJ':\n",
    "                candidate = candidate[:-1]\n",
    "            # Check if the last word is a noun or pronoun\n",
    "            for i in range(len(candidate)-1, -1, -1):\n",
    "                if candidate[i].pos_ in ['NOUN', 'PROPN']:\n",
    "                    return (candidate[:i+1])    \n",
    "        return candidate\n",
    "        \n",
    "    def keywords(self, text: str, n_keywords: int, min_words: int) -> List[Tuple[Tuple[str], int]]:\n",
    "        '''\n",
    "        Extract the top n most frequent keywords from the text.\n",
    "        Keywords are sequences of adjectives and nouns that end in a noun\n",
    "        \n",
    "        Arguments:\n",
    "            text       -- the raw text from which to extract keywords\n",
    "            n_keywords -- the number of keywords to return\n",
    "            min_words  -- the number of words a potential keyphrase has to include\n",
    "                          if this is set to 2, then only keyphrases consisting of 2+ words are counted\n",
    "        Returns:\n",
    "            keywords   -- List of keywords and their count, sorted by the count\n",
    "        '''\n",
    "        doc = self.nlp(text)\n",
    "        keywords = []\n",
    "        \n",
    "        # Make a dictionary of sentence-indices with\n",
    "        # their respective tokens\n",
    "        all_tokens = []  \n",
    "        sentences = dict()\n",
    "        c = 0\n",
    "        for sentence in doc.sents:\n",
    "            tokens = []\n",
    "            for token in sentence:\n",
    "                all_tokens.append(token)\n",
    "                tokens.append(token)\n",
    "            sentences[c] = tokens\n",
    "            c = c + 1\n",
    "        \n",
    "        # For each sentence, go through their tokens,\n",
    "        # append the tokens which are ADJ/NOUN/PROPN to \n",
    "        # a dictionary. The key of this dictionary is the \n",
    "        # position that the token occurs in the sentence\n",
    "        for key, sentence in sentences.items():\n",
    "            possible_keyword = dict()\n",
    "            for index, token in enumerate(sentence):\n",
    "                if token.pos_ in ['ADJ','NOUN', 'PROPN']:\n",
    "                    if index not in possible_keyword:\n",
    "                        possible_keyword[index] = token\n",
    "            \n",
    "            # Get all the positions of the valid tokens\n",
    "            # and check which positions are consecutive. \n",
    "            # Consecutively occuring keys will give keyphrases.\n",
    "            data = list(possible_keyword.keys())\n",
    "            \n",
    "            # REFERENCE: https://stackoverflow.com/questions/\n",
    "            # 7352684/how-to-find-the-groups-of-consecutive-elements-in-a-numpy-array\n",
    "            for array in np.split(data, np.where(np.diff(data) != 1)[0]+1):\n",
    "                candidate = []\n",
    "                for index in array:\n",
    "                    candidate.append(possible_keyword[index])\n",
    "                    \n",
    "                # Check if the candidate is valid and if it is\n",
    "                # convert each token to string and convert the \n",
    "                # list that contains each candidate into a tuple.\n",
    "                candidate = self.validate_keyphrase(candidate)\n",
    "                if candidate:\n",
    "                    candidate = [str(i) for i in candidate]\n",
    "                    keywords.append(tuple(candidate))\n",
    "        \n",
    "        # Check if the keywords/keyphrases are \n",
    "        # more than min_words limit.\n",
    "        for keyword in keywords[:]:\n",
    "            if len(keyword) < min_words:\n",
    "                keywords.remove(keyword)\n",
    "        \n",
    "        # Make a list of tuples. Each tuple has the\n",
    "        # keyword and it's number of occurances\n",
    "        keywords_return = []\n",
    "        for key, value in Counter(keywords).items():\n",
    "            keywords_return.append((key, value))\n",
    "        \n",
    "        return sorted(keywords_return, key=lambda x: x[1], reverse=True)[:n_keywords]\n",
    "\n",
    "    \n",
    "with open('/srv/shares/NLP/wiki_nlp.txt', 'r') as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "    \n",
    "keywords = POSKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=1)\n",
    "\n",
    "'''\n",
    "Expected output:\n",
    "The keyword ('words',) appears 353 times.\n",
    "The keyword ('text',) appears 342 times.\n",
    "The keyword ('example',) appears 263 times.\n",
    "The keyword ('word',) appears 231 times.\n",
    "The keyword ('natural', 'language', 'processing') appears 184 times.\n",
    "...\n",
    "'''\n",
    "for keyword in keywords:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "micro-throw",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b4561146a35861783b7ff33b6f88418",
     "grade": true,
     "grade_id": "test_POS_Keywords_A0",
     "locked": true,
     "points": 35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-tonight",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d41818baad631b02348797225ecb10e",
     "grade": false,
     "grade_id": "POS_Keywords_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### POS tag based extraction B) [4 points]\n",
    "\n",
    "Rerun the keyword extrator with a minimum word count of ```min_words=2``` and a keyword count of ```n_keywords=15```.\n",
    "\n",
    "Store this in the variable ```keywords_2```. Print the result.\n",
    "\n",
    "Make sure to convert the input text to lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "angry-checkout",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2c992b8ff2e84e9e54e9ec43532e0b3",
     "grade": false,
     "grade_id": "POS_Keywords_B",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keyword ('natural', 'language', 'processing') appears 184 times.\n",
      "The keyword ('computational', 'linguistics') appears 105 times.\n",
      "The keyword ('external', 'links') appears 101 times.\n",
      "The keyword ('machine', 'translation') appears 94 times.\n",
      "The keyword ('information', 'retrieval') appears 70 times.\n",
      "The keyword ('natural', 'language') appears 66 times.\n",
      "The keyword ('sentiment', 'analysis') appears 58 times.\n",
      "The keyword ('=', 'references') appears 56 times.\n",
      "The keyword ('text', 'mining') appears 55 times.\n",
      "The keyword ('artificial', 'intelligence') appears 49 times.\n",
      "The keyword ('word', 'sense', 'disambiguation') appears 47 times.\n",
      "The keyword ('computer', 'science') appears 36 times.\n",
      "The keyword ('machine', 'learning') appears 33 times.\n",
      "The keyword ('information', 'extraction') appears 33 times.\n",
      "The keyword ('speech', 'recognition') appears 31 times.\n"
     ]
    }
   ],
   "source": [
    "keywords_2 = POSKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=2)\n",
    "for keyword in keywords_2:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-cedar",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a766c838e3206e322155af855456f4b9",
     "grade": true,
     "grade_id": "test_POS_Keywords_B0",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-demographic",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dba1ccbd96865d0c64ca24b2dd8a3bff",
     "grade": false,
     "grade_id": "Stopword_Keywords_AStopword_Keywords_B_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Stop word based keyword extraction\n",
    "\n",
    "One approach to extract keywords is by splitting the text at the stop words. Then we count these potential keywords and output the top $n$ keywords. Make sure to only include words proper words. Here we define proper words as those words that match the regular expression ```r'\\b(\\W+|\\w+)\\b'```. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-upgrade",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8169155e9883ba38ca52d261d3e4b771",
     "grade": false,
     "grade_id": "Stopword_Keywords_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Stop word based keyword extraction A) [35 points]\n",
    "\n",
    "Complete the function ```keywords``` in the class ```StopWordKeywordExtractor```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "framed-italy",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5aac28b78aebbabdb0b077e292efc5c",
     "grade": false,
     "grade_id": "Stopword_Keywords_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keyword ('words',) appears 273 times.\n",
      "The keyword ('text',) appears 263 times.\n",
      "The keyword ('example',) appears 257 times.\n",
      "The keyword ('word',) appears 201 times.\n",
      "The keyword ('references',) appears 184 times.\n",
      "The keyword ('natural', 'language', 'processing') appears 165 times.\n",
      "The keyword ('n',) appears 160 times.\n",
      "The keyword ('use',) appears 151 times.\n",
      "The keyword ('set',) appears 144 times.\n",
      "The keyword ('language',) appears 123 times.\n",
      "The keyword ('t',) appears 120 times.\n",
      "The keyword ('documents',) appears 118 times.\n",
      "The keyword ('based',) appears 115 times.\n",
      "The keyword ('1',) appears 115 times.\n",
      "The keyword ('number',) appears 106 times.\n",
      "CPU times: user 25.1 s, sys: 3.05 s, total: 28.2 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "class StopWordKeywordExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up SpaCy in a more efficient way by disabling what we do not need\n",
    "        # This is the dependency parser (parser) and the named entity recognizer (ner)\n",
    "        self.nlp = spacy.load('/srv/shares/NLP/en_core_web_sm', disable=['ner', 'parser'])\n",
    "        # Add the sentencizer to quickly split our text into sentences\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        # Increase the maximum length of text SpaCy can parse in one go\n",
    "        self.nlp.max_length = 1500000\n",
    "        \n",
    "    def is_proper_word(self, token:str) -> bool:\n",
    "        '''\n",
    "        Checks if the word is a proper word by our definition\n",
    "        \n",
    "        Arguments:\n",
    "            token     -- The token as a string\n",
    "        Return:\n",
    "            is_proper -- True / False\n",
    "        '''\n",
    "        match = re.search(r'\\b(\\W+|\\w+)\\b', token)\n",
    "        return match and token == match[0] \n",
    "    \n",
    "    def keywords(self, text: str, n_keywords: int, min_words: int) -> List[Tuple[Tuple[str], int]]:\n",
    "        '''\n",
    "        Extract the top n most frequent keywords from the text.\n",
    "        Keywords are sequences of adjectives and nouns that end in a noun\n",
    "        \n",
    "        Arguments:\n",
    "            text       -- the raw text from which to extract keywords\n",
    "            n_keywords -- the number of keywords to return\n",
    "            min_words  -- the number of words a potential keyphrase has to include\n",
    "                          if this is set to 2, then only keyphrases consisting of 2+ words are counted\n",
    "        Returns:\n",
    "            keywords   -- List of keywords and their count, sorted by the count\n",
    "                          Example: [(('potato'), 12), (('potato', 'harvesting'), 9), ...]\n",
    "        '''\n",
    "        doc = self.nlp(text)\n",
    "        keywords = []   \n",
    "        keywords_sub = []\n",
    "        # Split each sentence once a stop word\n",
    "        # or a word which is not proper is encountered.\n",
    "        # This split sentence stored as keyword or a keyphrase\n",
    "        for sentence in doc.sents:\n",
    "            for token in sentence:\n",
    "                if token.is_stop or not self.is_proper_word(str(token)):\n",
    "                    keywords.append(tuple(keywords_sub))\n",
    "                    keywords_sub = []\n",
    "                else:\n",
    "                    keywords_sub.append(str(token))\n",
    "                    \n",
    "        # Check if the keywords/keyphrases are \n",
    "        # more than min_words limit.\n",
    "        for keyword in keywords[:]:\n",
    "            if len(keyword) < min_words:\n",
    "                keywords.remove(keyword)\n",
    "                \n",
    "        # Make a list of tuples. Each tuple has the\n",
    "        # keyword and it's number of occurances\n",
    "        keywords_return = []\n",
    "        for key, value in Counter(keywords).items():\n",
    "            if key:\n",
    "                keywords_return.append((key, value))\n",
    "        \n",
    "        return sorted(keywords_return, key=lambda x: x[1], reverse=True)[:n_keywords]\n",
    "        \n",
    "with open('/srv/shares/NLP/wiki_nlp.txt', 'r') as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "    \n",
    "keywords = StopWordKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=1)\n",
    "\n",
    "'''\n",
    "Expected output:\n",
    "The keyword ('words',) appears 273 times.\n",
    "The keyword ('text',) appears 263 times.\n",
    "The keyword ('example',) appears 257 times.\n",
    "The keyword ('word',) appears 201 times.\n",
    "The keyword ('references',) appears 184 times.\n",
    "The keyword ('natural', 'language', 'processing') appears 165 times.\n",
    "...\n",
    "'''\n",
    "for keyword in keywords:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-webmaster",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "404059ce742f52d7a22c938834f96820",
     "grade": true,
     "grade_id": "test_Stopword_Keywords_A0",
     "locked": true,
     "points": 35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-dispute",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b86c7fb9d2d8317a317e25292586a04",
     "grade": false,
     "grade_id": "Stopword_Keywords_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Stop word based keyword extraction B) [4 points]\n",
    "\n",
    "Rerun the keyword extrator with a minimum word count of ```min_words=2``` and a keyword count of ```n_keywords=15```.\n",
    "\n",
    "Store this in the variable ```keywords_2```. Print the result.\n",
    "\n",
    "Make sure to convert the input text to lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "restricted-violation",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ace49c86890f478366272ba68cc1270",
     "grade": false,
     "grade_id": "Stopword_Keywords_B",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keyword ('natural', 'language', 'processing') appears 165 times.\n",
      "The keyword ('computational', 'linguistics') appears 103 times.\n",
      "The keyword ('external', 'links') appears 101 times.\n",
      "The keyword ('machine', 'translation') appears 68 times.\n",
      "The keyword ('information', 'retrieval') appears 67 times.\n",
      "The keyword ('natural', 'language') appears 47 times.\n",
      "The keyword ('text', 'mining') appears 47 times.\n",
      "The keyword ('sentiment', 'analysis') appears 45 times.\n",
      "The keyword ('word', 'sense', 'disambiguation') appears 45 times.\n",
      "The keyword ('artificial', 'intelligence') appears 45 times.\n",
      "The keyword ('machine', 'learning') appears 42 times.\n",
      "The keyword ('computer', 'science') appears 34 times.\n",
      "The keyword ('speech', 'recognition') appears 29 times.\n",
      "The keyword ('information', 'extraction') appears 29 times.\n",
      "The keyword ('customer', 'inserts') appears 29 times.\n"
     ]
    }
   ],
   "source": [
    "keywords_2 = StopWordKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=2)\n",
    "\n",
    "for keyword in keywords_2:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-occurrence",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2557e9fa64432c99b589af4dc730fe3b",
     "grade": true,
     "grade_id": "test_Stopword_Keywords_B0",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
